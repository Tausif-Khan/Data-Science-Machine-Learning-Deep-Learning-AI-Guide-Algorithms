{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Processing (NLP).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "96g1Opq2ObBu",
        "_MHy51afObCD",
        "YDATTLs8ObDw",
        "3D3hMYdWosj5",
        "GcXic02dObBr"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myrondza/Data-Science-Machine-Learning-Deep-Learning-AI-Guide-Algorithms/blob/master/Natural_Language_Processing_(NLP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z_cGCgDrObBN"
      },
      "source": [
        "# Natural Language Processing (NLP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "864O-bK8ObBO",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    input='content',\n",
        "    encoding='utf-8',\n",
        "    decode_error='strict',\n",
        "    strip_accents=None,\n",
        "    lowercase=True,\n",
        "    preprocessor=None,\n",
        "    tokenizer=None,\n",
        "    stop_words=None,\n",
        "    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "    ngram_range=(1, 1),\n",
        "    analyzer='word',\n",
        "    max_df=1.0,\n",
        "    min_df=1,\n",
        "    max_features=None,\n",
        "    vocabulary=None,\n",
        "    binary=False,\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ym76sSuBObBQ",
        "outputId": "e71746bf-e9be-4b0d-b1d6-4a9dae2b29fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "corpus = ['This is an example of NLP',\n",
        "          'This is the first document.',\n",
        "          'And the second one.',\n",
        "          'Is this the first document?']\n",
        "\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "X    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<4x12 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 20 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mXCp3TN6ObBU",
        "outputId": "86810646-5489-415d-b0c6-cfc5e0a6780d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "analyze = vectorizer.build_analyzer()\n",
        "analyze(\"This is a text document to analyze.\") == (['this', 'is', 'text', 'document', 'analyze'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OftttK-PObBW",
        "outputId": "5358770f-3cbf-473d-9104-c4ee9f8aacfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "vectorizer.get_feature_names()\n",
        "X.toarray()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1],\n",
              "       [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1],\n",
              "       [0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0],\n",
              "       [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x1Dh_cpwObBY",
        "outputId": "e2565d8b-9395-4bc5-ac3d-4039d2402435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vectorizer.transform(['Something completely new.']).toarray()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6MewG2mHObBb",
        "outputId": "471a76db-fcbd-40f8-f028-b06baef20d5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\n",
        "                                     token_pattern=r'\\b\\w+\\b', min_df=1)\n",
        "analyze = bigram_vectorizer.build_analyzer()\n",
        "analyze('Bi-grams are cool!') == (['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rqg0t56yObBe",
        "outputId": "6fb6a8bc-f3e1-41e4-f8b5-548be9dd1a88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "X2 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
        "X2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "        1, 1, 0],\n",
              "       [0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "        1, 1, 0],\n",
              "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
              "        0, 0, 0],\n",
              "       [0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "        1, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "61OekX4MObBj",
        "outputId": "72ef1934-21d8-4a97-f7a7-7e229c66acdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "transformer = TfidfTransformer(smooth_idf=False)\n",
        "\n",
        "TfidfTransformer(norm='l2', smooth_idf=False, sublinear_tf=False,\n",
        "                 use_idf=True)\n",
        "\n",
        "tfidf = transformer.fit_transform(X.toarray())\n",
        "tfidf.toarray() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.46714844, 0.        , 0.        , 0.46714844, 0.        ,\n",
              "        0.25208067, 0.46714844, 0.46714844, 0.        , 0.        ,\n",
              "        0.        , 0.25208067],\n",
              "       [0.        , 0.        , 0.51741994, 0.        , 0.51741994,\n",
              "        0.3935112 , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.3935112 , 0.3935112 ],\n",
              "       [0.        , 0.55121857, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.55121857, 0.55121857,\n",
              "        0.29744623, 0.        ],\n",
              "       [0.        , 0.        , 0.51741994, 0.        , 0.51741994,\n",
              "        0.3935112 , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.3935112 , 0.3935112 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tCf6J99mObBn",
        "outputId": "654f1a4f-e363-4b9c-a0f7-04118af73966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))\n",
        "counts = ngram_vectorizer.fit_transform(['words', 'wprds'])\n",
        "ngram_vectorizer.get_feature_names() == ([' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'])\n",
        "\n",
        "counts.toarray().astype(int)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 0, 1, 1, 1, 0],\n",
              "       [1, 1, 0, 1, 1, 1, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CGuNGLTCObBp",
        "outputId": "1e2ebd2f-bb4d-47b1-ad28-3d819b0564b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))\n",
        "ngram_vectorizer.fit_transform(['jumpy fox'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x4 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 4 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "96g1Opq2ObBu"
      },
      "source": [
        "## Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SXGi9aZSObBv",
        "outputId": "c1cef993-06a6-4e4f-c9df-b359ed16cf00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        }
      },
      "source": [
        "!pip install spacy\n",
        "import spacy\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "\n",
        "nlp = spacy.load(\"en\")\n",
        "\n",
        "# Process whole documents\n",
        "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
        "        \"Google in 2007, few people outside of the company took him \"\n",
        "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
        "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
        "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
        "        \"this week.\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Analyze syntax\n",
        "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
        "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
        "\n",
        "# Find named entities, phrases and concepts\n",
        "\n",
        "for entity in doc.ents:\n",
        "    print(entity.text, entity.label_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.4)\n",
            "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.6.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy) (4.28.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
            "Verbs: ['start', 'work', 'drive', 'take', 'can', 'tell', 'would', 'shake', 'turn', 'be', 'talk', 'say']\n",
            "Sebastian Thrun PERSON\n",
            "Google ORG\n",
            "2007 DATE\n",
            "American NORP\n",
            "Thrun PERSON\n",
            "Recode ORG\n",
            "earlier this week DATE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ax8C7TJEObBy",
        "outputId": "a92a702a-64ba-48fe-8d69-ba216dfc92b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en\")\n",
        "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple 0 5 ORG\n",
            "U.K. 27 31 GPE\n",
            "$1 billion 44 54 MONEY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vqKXW_CdObBz",
        "outputId": "0c087014-06f6-4688-957b-c92299ffba3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en\")\n",
        "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple\n",
            "is\n",
            "looking\n",
            "at\n",
            "buying\n",
            "U.K.\n",
            "startup\n",
            "for\n",
            "$\n",
            "1\n",
            "billion\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YH4TLd5XObB1",
        "outputId": "95d26ff9-f155-490e-83c0-0f404fb3bbef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "\n",
        "prefix_re = re.compile(r'''^[[(\"']''')\n",
        "suffix_re = re.compile(r'''[])\"']$''')\n",
        "infix_re = re.compile(r'''[-~]''')\n",
        "simple_url_re = re.compile(r'''^https?://''')\n",
        "\n",
        "def custom_tokenizer(nlp):\n",
        "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
        "                                suffix_search=suffix_re.search,\n",
        "                                infix_finditer=infix_re.finditer,\n",
        "                                token_match=simple_url_re.match)\n",
        "\n",
        "nlp = spacy.load(\"en\")\n",
        "\n",
        "nlp.tokenizer = custom_tokenizer(nlp)\n",
        "doc = nlp(u\"hello-world.\")\n",
        "\n",
        "print([t.text for t in doc])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello', '-', 'world.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7XkQhjFsObB3",
        "outputId": "15285b24-cc7e-4745-edd4-92787218406b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en\")\n",
        "doc = nlp(\"I live in New York\")\n",
        "\n",
        "print(\"Before:\", [token.text for token in doc])\n",
        "\n",
        "with doc.retokenize() as retokenizer:\n",
        "    retokenizer.merge(doc[3:5], attrs={\"LEMMA\": \"new york\"})\n",
        "    \n",
        "print(\"After:\", [token.text for token in doc])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before: ['I', 'live', 'in', 'New', 'York']\n",
            "After: ['I', 'live', 'in', 'New York']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O-8ZcdbCObB6",
        "outputId": "12d2f7d5-1274-45f8-b851-7d8f17a5a180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en\")\n",
        "doc = nlp(u\"This is a sentence. This is another sentence.\")\n",
        "\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a sentence.\n",
            "This is another sentence.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vVqamYu8ObB8",
        "outputId": "6bab806c-e59c-4ba3-b25d-8ed25d7037fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from spacy.lang.en import English\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = English()  # We only want the tokenizer, so no need to load a model\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pos_emoji = [u\"😀\", u\"😃\", u\"😂\", u\"🤣\", u\"😊\", u\"😍\"]  # Positive emoji\n",
        "neg_emoji = [u\"😞\", u\"😠\", u\"😩\", u\"😢\", u\"😭\", u\"😒\"]  # Negative emoji\n",
        "\n",
        "# Add patterns to match one or more emoji tokens\n",
        "pos_patterns = [[{\"ORTH\": emoji}] for emoji in pos_emoji]\n",
        "neg_patterns = [[{\"ORTH\": emoji}] for emoji in neg_emoji]\n",
        "\n",
        "# Function to label the sentiment\n",
        "def label_sentiment(matcher, doc, i, matches):\n",
        "    match_id, start, end = matches[i]\n",
        "    if doc.vocab.strings[match_id] == \"HAPPY\":  # Don't forget to get string!\n",
        "        doc.sentiment += 0.1  # Add 0.1 for positive sentiment\n",
        "    elif doc.vocab.strings[match_id] == \"SAD\":\n",
        "        doc.sentiment -= 0.1  # Subtract 0.1 for negative sentiment\n",
        "\n",
        "matcher.add(\"HAPPY\", label_sentiment, *pos_patterns)  # Add positive pattern\n",
        "matcher.add(\"SAD\", label_sentiment, *neg_patterns)  # Add negative pattern\n",
        "\n",
        "# Add pattern for valid hashtag, i.e. '#' plus any ASCII token\n",
        "matcher.add(\"HASHTAG\", None, [{\"ORTH\": \"#\"}, {\"IS_ASCII\": True}])\n",
        "\n",
        "doc = nlp(u\"Hello world 😀 #MondayMotivation\")\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    string_id = doc.vocab.strings[match_id]  # Look up string ID\n",
        "    span = doc[start:end]\n",
        "    print(string_id, span.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HAPPY 😀\n",
            "HASHTAG #MondayMotivation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "omAbFScpObB-",
        "outputId": "5f6263e1-6f77-4b94-db1c-494531accdbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en\")\n",
        "doc = nlp(\"Dr Alex Smith chaired first board meeting of Acme Corp Inc.\")\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Alex Smith', 'PERSON'), ('first', 'ORDINAL'), ('Acme Corp Inc.', 'ORG')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l-U3ecwVObCA",
        "outputId": "12a35b94-c64e-4772-bbeb-9e2c89e7861d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "import spacy\n",
        "from spacy.pipeline import merge_entities\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load(\"en\")\n",
        "\n",
        "def extract_person_orgs(doc):\n",
        "    person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
        "    for ent in person_entities:\n",
        "        head = ent.root.head\n",
        "        if head.lemma_ == \"work\":\n",
        "            preps = [token for token in head.children if token.dep_ == \"prep\"]\n",
        "            for prep in preps:\n",
        "                orgs = [token for token in prep.children if token.ent_type_ == \"ORG\"]\n",
        "                print({'person': ent, 'orgs': orgs, 'past': head.tag_ == \"VBD\"})\n",
        "    return doc\n",
        "\n",
        "# To make the entities easier to work with, we'll merge them into single tokens\n",
        "nlp.add_pipe(merge_entities)\n",
        "nlp.add_pipe(extract_person_orgs)\n",
        "\n",
        "doc = nlp(\"Alex Smith worked at Acme Corp Inc.\")\n",
        "# If you're not in a Jupyter / IPython environment, use displacy.serve\n",
        "displacy.render(doc, options={'fine_grained': True})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'person': Alex Smith, 'orgs': [Acme Corp Inc.], 'past': True}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"61ef71ef3d4248f4996491a946b1ace0-0\" class=\"displacy\" width=\"750\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Alex Smith</tspan>\\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NNP</tspan>\\n</text>\\n\\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">worked</tspan>\\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VBD</tspan>\\n</text>\\n\\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">at</tspan>\\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">IN</tspan>\\n</text>\\n\\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Acme Corp Inc.</tspan>\\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NNP</tspan>\\n</text>\\n\\n<g class=\"displacy-arrow\">\\n    <path class=\"displacy-arc\" id=\"arrow-61ef71ef3d4248f4996491a946b1ace0-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\\n        <textPath xlink:href=\"#arrow-61ef71ef3d4248f4996491a946b1ace0-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\\n    </text>\\n    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\\n</g>\\n\\n<g class=\"displacy-arrow\">\\n    <path class=\"displacy-arc\" id=\"arrow-61ef71ef3d4248f4996491a946b1ace0-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\\n        <textPath xlink:href=\"#arrow-61ef71ef3d4248f4996491a946b1ace0-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\\n    </text>\\n    <path class=\"displacy-arrowhead\" d=\"M400.0,91.5 L408.0,79.5 392.0,79.5\" fill=\"currentColor\"/>\\n</g>\\n\\n<g class=\"displacy-arrow\">\\n    <path class=\"displacy-arc\" id=\"arrow-61ef71ef3d4248f4996491a946b1ace0-0-2\" stroke-width=\"2px\" d=\"M420,89.5 C420,2.0 575.0,2.0 575.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\\n        <textPath xlink:href=\"#arrow-61ef71ef3d4248f4996491a946b1ace0-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\\n    </text>\\n    <path class=\"displacy-arrowhead\" d=\"M575.0,91.5 L583.0,79.5 567.0,79.5\" fill=\"currentColor\"/>\\n</g>\\n</svg>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bublqtX5ObCB",
        "outputId": "71b86262-9b5e-492b-b530-7392a054c20c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "text = \"\"\"But Google is starting from behind. The company made a late push\n",
        "into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa\n",
        "software, which runs on its Echo and Dot devices, have clear leads in\n",
        "consumer adoption.\"\"\"\n",
        "\n",
        "nlp = spacy.load(\"en\")\n",
        "doc = nlp(text)\n",
        "displacy.serve(doc, style=\"ent\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using the 'ent' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n",
            "Shutting down server on port 5000.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQDOoj9NA2fB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.matcher import Matcher, PhraseMatcher\n",
        "import spacy\n",
        "nlp = spacy.load(\"en\") \n",
        "nlp = spacy.load(\"en_core_web_sm\") \n",
        "\n",
        "def on_match(matcher, doc, id, matches):\n",
        "    print('Matched!', matches)\n",
        "\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "assert \"Food\" not in matcher\n",
        "matcher.add(\"Food\", on_match,nlp(u\"cuisine\"),nlp(u\"beverages\"),nlp(u\"fruits\"),nlp(u\"Bread rolls\"),nlp(u\"breakfasts\"),nlp(u\"custard\"),nlp(u\"fish\"),nlp(u\"Drinks\"),nlp(u\"Food\"),nlp(u\"snack\"),nlp(u\"Water\"),nlp(u\"food\"),nlp(u\"snacks\"),nlp(u\"lunch\"),nlp(u\"drinks\"),nlp(u\"dinner\"),nlp(u\"meal\"),nlp(u\"meals\"),nlp(u\"dairy\"),nlp(u\"icecream\"),nlp(u\"noodles\"),nlp(u\"beer\"),nlp(u\"wine\"),nlp(u\"Drink\"),nlp(u\"lamb\"),nlp(u\"water\"))\n",
        "assert \"Food\" in matcher"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6gTJT2XBWE5",
        "colab_type": "code",
        "outputId": "36020fe5-4077-44e2-c396-f5af3f257981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "doc = nlp(u\"The airlines provided us with great food food and snacks\")\n",
        "matches=matcher(doc)\n",
        "\n",
        "print(\"Key for matched words : \",matches)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matched! [(13829306678568089077, 6, 7), (13829306678568089077, 7, 8), (13829306678568089077, 9, 10)]\n",
            "Matched! [(13829306678568089077, 6, 7), (13829306678568089077, 7, 8), (13829306678568089077, 9, 10)]\n",
            "Matched! [(13829306678568089077, 6, 7), (13829306678568089077, 7, 8), (13829306678568089077, 9, 10)]\n",
            "Key for matched words :  [(13829306678568089077, 6, 7), (13829306678568089077, 7, 8), (13829306678568089077, 9, 10)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_MHy51afObCD"
      },
      "source": [
        "## NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Oc0_AxXZObCD",
        "outputId": "2545e0eb-4895-467f-9066-6397b4dfc6a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "   \n",
        "ps = PorterStemmer() \n",
        "  \n",
        "# choose some words to be stemmed \n",
        "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n",
        "  \n",
        "for w in words: \n",
        "    print(w, \" : \", ps.stem(w)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "program  :  program\n",
            "programs  :  program\n",
            "programer  :  program\n",
            "programing  :  program\n",
            "programers  :  program\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "py6XbQ36ObCF",
        "outputId": "371a0ec6-575e-4c16-9706-80132169eb4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "   \n",
        "ps = PorterStemmer() \n",
        "   \n",
        "sentence = \"Programers program with programing languages\"\n",
        "words = word_tokenize(sentence) \n",
        "   \n",
        "for w in words: \n",
        "    print(w, \" : \", ps.stem(w)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Programers  :  program\n",
            "program  :  program\n",
            "with  :  with\n",
            "programing  :  program\n",
            "languages  :  languag\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JTOYTn3QObCK",
        "outputId": "0bd30b59-8ff3-4643-d073-72de52ad4578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import random \n",
        "from nltk.corpus import names \n",
        "import nltk \n",
        "nltk.download('names')\n",
        "  \n",
        "def gender_features(word): \n",
        "    return {'last_letter':word[-1]} \n",
        "  \n",
        "# preparing a list of examples and corresponding class labels. \n",
        "labeled_names = ([(name, 'male') for name in names.words('male.txt')]+\n",
        "             [(name, 'female') for name in names.words('female.txt')]) \n",
        "  \n",
        "random.shuffle(labeled_names) \n",
        "  \n",
        "# we use the feature extractor to process the names data. \n",
        "featuresets = [(gender_features(n), gender)  \n",
        "               for (n, gender)in labeled_names] \n",
        "  \n",
        "# Divide the resulting list of feature \n",
        "# sets into a training set and a test set. \n",
        "train_set, test_set = featuresets[500:], featuresets[:500] \n",
        "  \n",
        "# The training set is used to  \n",
        "# train a new \"naive Bayes\" classifier. \n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set) \n",
        "  \n",
        "print(classifier.classify(gender_features('Myron'))) \n",
        "  \n",
        "# output should be 'male' \n",
        "print(nltk.classify.accuracy(classifier, train_set)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n",
            "male\n",
            "0.7634336378291241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YDCHJb5iObCM",
        "outputId": "429e5ad1-7e6b-408c-858b-591d2a65448d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "classifier.show_most_informative_features(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "             last_letter = 'a'            female : male   =     36.5 : 1.0\n",
            "             last_letter = 'k'              male : female =     32.4 : 1.0\n",
            "             last_letter = 'f'              male : female =     16.1 : 1.0\n",
            "             last_letter = 'p'              male : female =     11.3 : 1.0\n",
            "             last_letter = 'v'              male : female =     10.6 : 1.0\n",
            "             last_letter = 'd'              male : female =     10.1 : 1.0\n",
            "             last_letter = 'm'              male : female =      9.6 : 1.0\n",
            "             last_letter = 'o'              male : female =      8.6 : 1.0\n",
            "             last_letter = 'r'              male : female =      6.6 : 1.0\n",
            "             last_letter = 'w'              male : female =      5.4 : 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bTAo2IYbObDv",
        "outputId": "fb20226c-d8a0-4c85-851a-0e53e8ec209d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "import nltk\n",
        "nltk.download('wordnet')  \n",
        "lemmatizer = WordNetLemmatizer() \n",
        "  \n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
        "  \n",
        "# a denotes adjective in \"pos\" \n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\")) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "rocks : rock\n",
            "corpora : corpus\n",
            "better : good\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YDATTLs8ObDw"
      },
      "source": [
        "## TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zs-4OueQObDx",
        "outputId": "9eabe4ce-a22e-4a5f-e9c3-4338fd2a311f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('brown')\n",
        " \n",
        "text = \"The service was good\"\n",
        "\n",
        "blob = TextBlob(text)\n",
        "blob.tags          \n",
        "\n",
        "blob.noun_phrases   \n",
        "for sentence in blob.sentences:\n",
        "    print(sentence.sentiment.polarity)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g3eXb6KdObDz",
        "outputId": "6dd3d554-a948-4d56-f628-91d734ac6c35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from textblob import Word\n",
        "w = Word('falibility')\n",
        "w.spellcheck()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('fallibility', 1.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "43WlBL-tObD0",
        "outputId": "bdc7e445-7aa0-4d7b-e5db-c84e63d2f4af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "en_blob = TextBlob(u'Good Morning')\n",
        "print(en_blob.translate(to='es'))\n",
        "print(en_blob.translate(to='fr'))\n",
        "print(en_blob.translate(to='zh-CN'))\n",
        "print(en_blob.translate(to='ar'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Buenos días\n",
            "Bonjour\n",
            "早上好\n",
            "صباح الخير\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NPB4VPnyObD2",
        "outputId": "d6a47109-a964-4be7-ec4d-22ea480ee208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "b = TextBlob(u\"بسيط هو أفضل من مجمع\")\n",
        "b.detect_language()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ar'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0UjbZuKJObD4",
        "outputId": "13b2ca05-d818-4871-f973-e4636b875520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "blob = TextBlob(\"Now is better than never.\")\n",
        "blob.ngrams(n=3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['Now', 'is', 'better']),\n",
              " WordList(['is', 'better', 'than']),\n",
              " WordList(['better', 'than', 'never'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "77avOTNWObD6",
        "outputId": "3f181dbe-a60e-4940-c666-7c1cd327a172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from textblob import Word\n",
        "from textblob.wordnet import VERB\n",
        "word = Word(\"octopus\")\n",
        "word.synsets\n",
        "Word(\"hack\").get_synsets(pos=VERB)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('chop.v.05'),\n",
              " Synset('hack.v.02'),\n",
              " Synset('hack.v.03'),\n",
              " Synset('hack.v.04'),\n",
              " Synset('hack.v.05'),\n",
              " Synset('hack.v.06'),\n",
              " Synset('hack.v.07'),\n",
              " Synset('hack.v.08')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n6h9Ma-3ObD8",
        "outputId": "1f14b50c-f2a4-4587-d970-57c0755ef702",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        " Word(\"rain\").definitions"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['water falling in drops from vapor condensed in the atmosphere',\n",
              " 'drops of fresh water that fall as precipitation from clouds',\n",
              " 'anything happening rapidly or in quick successive',\n",
              " 'precipitate as rain']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D3hMYdWosj5",
        "colab_type": "text"
      },
      "source": [
        "## Google Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FceEkwcco1TP",
        "colab_type": "code",
        "outputId": "cad05474-0571-4c21-cb74-2427dbf3ee83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#!pip install googletrans\n",
        "from googletrans import Translator\n",
        "translator = Translator(service_urls=[\n",
        "      'translate.google.com',\n",
        "      'translate.google.co.uk',\n",
        "      'translate.google.co.in'\n",
        "    ])\n",
        "translations = translator.translate(['صباح الخير', '早上好', 'Buenos días'], dest='en')\n",
        "for translation in translations:\n",
        "    print(translation.origin, ' -> ', translation.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "صباح الخير  ->  good morning\n",
            "早上好  ->  Good morning\n",
            "Buenos días  ->  Good Morning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJxGQ-gipENU",
        "colab_type": "code",
        "outputId": "46b6d2b0-efab-47d7-c450-8668b111851a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from googletrans import Translator\n",
        "translator = Translator(service_urls=[\n",
        "      'translate.google.com',\n",
        "      'translate.google.co.uk',\n",
        "      'translate.google.co.in'\n",
        "    ])\n",
        "translations = translator.translate('안녕하세요', dest='ja')\n",
        "print(translations)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Translated(src=ko, dest=ja, text=こんにちは, pronunciation=Kon'nichiwa, extra_data=\"{'translat...\")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGMBFv5DpFuo",
        "colab_type": "code",
        "outputId": "25e8cd9a-085b-41be-d285-e06d5156be99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a = translator.detect('안녕하세요')\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detected(lang=ko, confidence=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szu7Ovv2pJ0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def google_translator(x):\n",
        "    translator = Translator()\n",
        "    translations=translator.translate(x, dest='en')\n",
        "    return translations.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLMJuG20pNWR",
        "colab_type": "code",
        "outputId": "19d17364-bad0-4601-ca2e-cd04f7a01a29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a = google_translator('안녕하세요')\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJkIU1HT_VNM",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GcXic02dObBr"
      },
      "source": [
        "### Vader Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xhg1U3L-ObBs",
        "outputId": "4db3b508-782b-4631-9ed5-889fcd2bfb97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!pip install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sentences = [\"The food was good.\",\n",
        "             \"The service was not very good!\", \n",
        "             \"Not bad at all\",\n",
        "             \"The service was horrible\"]\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "for sentence in sentences:\n",
        "    vs = analyzer.polarity_scores(sentence)\n",
        "    print(\"{:-<50} {}\".format(sentence, str(vs)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.6/dist-packages (3.2.1)\n",
            "The food was good.-------------------------------- {'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}\n",
            "The service was not very good!-------------------- {'neg': 0.368, 'neu': 0.632, 'pos': 0.0, 'compound': -0.4432}\n",
            "Not bad at all------------------------------------ {'neg': 0.0, 'neu': 0.513, 'pos': 0.487, 'compound': 0.431}\n",
            "The service was horrible-------------------------- {'neg': 0.538, 'neu': 0.462, 'pos': 0.0, 'compound': -0.5423}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDoEw_SUnyz9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "5344aee7-4a2f-473a-bb60-d809fead92ec"
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sentences = [\"The food was good. 😍\",\n",
        "             \"The service was not very good 😩 \", \n",
        "             \"Not bad at all 😀\",\n",
        "             \"The service was horrible 😢\"]\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "for sentence in sentences:\n",
        "    vs = analyzer.polarity_scores(sentence)\n",
        "    print(\"{:-<50} {}\".format(sentence, str(vs)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The food was good. 😍------------------------------ {'neg': 0.0, 'neu': 0.504, 'pos': 0.496, 'compound': 0.7096}\n",
            "The service was not very good 😩 ------------------ {'neg': 0.246, 'neu': 0.564, 'pos': 0.19, 'compound': -0.1538}\n",
            "Not bad at all 😀---------------------------------- {'neg': 0.0, 'neu': 0.428, 'pos': 0.572, 'compound': 0.6542}\n",
            "The service was horrible 😢------------------------ {'neg': 0.623, 'neu': 0.377, 'pos': 0.0, 'compound': -0.765}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGlURhB-nwhn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "b91daf6f-e670-40e7-ab6b-aa5ac8c5d4c8"
      },
      "source": [
        "!pip install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sentences = [\"😀\",\"🤣\",\"😩\",\"😭\",\"😍\",\"😒\",\"😢\"]\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "for sentence in sentences:\n",
        "    vs = analyzer.polarity_scores(sentence)\n",
        "    print(\"{:-<50} {}\".format(sentence, str(vs)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/9e/c53e1fc61aac5ee490a6ac5e21b1ac04e55a7c2aba647bb8411c9aadf24e/vaderSentiment-3.2.1-py2.py3-none-any.whl (125kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.9MB/s \n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.2.1\n",
            "😀------------------------------------------------- {'neg': 0.0, 'neu': 0.286, 'pos': 0.714, 'compound': 0.3612}\n",
            "🤣------------------------------------------------- {'neg': 0.0, 'neu': 0.556, 'pos': 0.444, 'compound': 0.4939}\n",
            "😩------------------------------------------------- {'neg': 0.677, 'neu': 0.323, 'pos': 0.0, 'compound': -0.2732}\n",
            "😭------------------------------------------------- {'neg': 0.608, 'neu': 0.392, 'pos': 0.0, 'compound': -0.4767}\n",
            "😍------------------------------------------------- {'neg': 0.0, 'neu': 0.5, 'pos': 0.5, 'compound': 0.4588}\n",
            "😒------------------------------------------------- {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "😢------------------------------------------------- {'neg': 0.756, 'neu': 0.244, 'pos': 0.0, 'compound': -0.4767}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS5Le3FHo7j4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}