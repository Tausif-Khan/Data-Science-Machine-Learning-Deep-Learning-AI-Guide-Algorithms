{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Microsoft SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymssql\n",
    "\n",
    "server = getenv(\"xxx.xxx.xx.xx\")\n",
    "user = getenv(\"username\")\n",
    "password = getenv(\"password\")\n",
    "\n",
    "conn = pymssql.connect(server, user, password, \"tempdb\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "                        IF OBJECT_ID('persons', 'U') IS NOT NULL\n",
    "                            DROP TABLE persons\n",
    "                        CREATE TABLE persons (\n",
    "                            id INT NOT NULL,\n",
    "                            name VARCHAR(100),\n",
    "                            salesrep VARCHAR(100),\n",
    "                            PRIMARY KEY(id)\n",
    "                        )\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "cursor.executemany(\n",
    "    \"INSERT INTO persons VALUES (%d, %s, %s)\",\n",
    "    [(1, 'John Smith', 'John Doe'),\n",
    "     (2, 'Jane Doe', 'Joe Dog'),\n",
    "     (3, 'Mike T.', 'Sarah H.')])\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "cursor.execute('SELECT * FROM persons')\n",
    "row = cursor.fetchone()\n",
    "while row:\n",
    "    print(\"ID=%d, Name=%s\" % (row[0], row[1]))\n",
    "    row = cursor.fetchone()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TM1 Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TM1py import TM1Service\n",
    "ADDRESS = 'xxx.xxx.xx.xx'\n",
    "PORT = 12354\n",
    "USER = 'admin'\n",
    "PASSWORD = 'apple'\n",
    "SSL = False\n",
    "DECODE_B64 = False\n",
    "\n",
    "CUBE = \"\"\n",
    "VIEW = \"\"\n",
    "\n",
    "with TM1Service(address=ADDRESS, port=PORT, user=USER, password=PASSWORD, ssl=SSL, decode_b64=DECODE_B64) as tm1:\n",
    "    df = tm1.cubes.cells.execute_view_dataframe_pivot(\n",
    "        cube_name=CUBE,\n",
    "        view_name=VIEW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Data Frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3        1.0         1180   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
       "0      5650     1.0           0     0  ...      7        1180              0   \n",
       "\n",
       "   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "0      1955             0    98178  47.5112 -122.257           1340   \n",
       "\n",
       "   sqft_lot15  \n",
       "0        5650  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'kc_house_data.csv', error_bad_lines=False)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"bedrooms\",\"sqft_living\",\"floors\",\"bathrooms\"]]\n",
    "y = df[\"price\"]\n",
    "#y = df[\"waterfront\"]\n",
    "#y = df[\"grade\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14480, 4)\n",
      "(14480,)\n",
      "(7133, 4)\n",
      "(7133,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score : 0.5025801802472102\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score Uniform Average : 0.5025801802472102\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Maximum Error (ME) : 4190853.198694827\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Absolute Error (MAE) : 174286.3120128402\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Error (MSE) : 74492754485.58958\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Log Error (MSLE) : 0.1582498451241946\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Median Absolute Error (MedAE) : 126220.20859889686\n",
      "--------------------------------------------------------------------------------------------------\n",
      "R2 Score (R2) : 0.5024906747145044\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import max_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "linreg = LinearRegression(\n",
    "    fit_intercept=True,\n",
    "    normalize=False,\n",
    "    copy_X=True,\n",
    "    n_jobs=None,\n",
    ")\n",
    "\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linreg.predict(X_test)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score :\",explained_variance_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score Uniform Average :\",explained_variance_score(y_test, y_pred, multioutput='uniform_average'))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Maximum Error (ME) :\",max_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Absolute Error (MAE) :\",mean_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Error (MSE) :\",mean_squared_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Log Error (MSLE) :\",mean_squared_log_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Median Absolute Error (MedAE) :\",median_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"R2 Score (R2) :\",r2_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score : 0.5025798594802964\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score Uniform Average : 0.5025798594802964\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Maximum Error (ME) : 4190861.3067016546\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Absolute Error (MAE) : 174286.19780452116\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Error (MSE) : 74492802611.21843\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Log Error (MSLE) : 0.15824916448285217\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Median Absolute Error (MedAE) : 126217.24480640457\n",
      "--------------------------------------------------------------------------------------------------\n",
      "R2 Score (R2) : 0.5024903533013776\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "linrid = linear_model.Ridge(\n",
    "    alpha=1.0,\n",
    "    fit_intercept=True,\n",
    "    normalize=False,\n",
    "    copy_X=True,\n",
    "    max_iter=None,\n",
    "    tol=0.001,\n",
    "    solver='auto',\n",
    "    random_state=None,\n",
    ")\n",
    "linrid.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linrid.predict(X_test)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score :\",explained_variance_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score Uniform Average :\",explained_variance_score(y_test, y_pred, multioutput='uniform_average'))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Maximum Error (ME) :\",max_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Absolute Error (MAE) :\",mean_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Error (MSE) :\",mean_squared_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Log Error (MSLE) :\",mean_squared_log_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Median Absolute Error (MedAE) :\",median_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"R2 Score (R2) :\",r2_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score : 0.5025802829016262\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score Uniform Average : 0.5025802829016262\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Maximum Error (ME) : 4190849.4994220138\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Absolute Error (MAE) : 174286.10658582946\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Error (MSE) : 74492739745.50328\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Log Error (MSLE) : 0.15824910236510256\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Median Absolute Error (MedAE) : 126215.813073406\n",
      "--------------------------------------------------------------------------------------------------\n",
      "R2 Score (R2) : 0.5024907731580435\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "linlass = linear_model.Lasso(\n",
    "    alpha=1.0,\n",
    "    fit_intercept=True,\n",
    "    normalize=False,\n",
    "    precompute=False,\n",
    "    copy_X=True,\n",
    "    max_iter=1000,\n",
    "    tol=0.0001,\n",
    "    warm_start=False,\n",
    "    positive=False,\n",
    "    random_state=None,\n",
    "    selection='cyclic',\n",
    ")\n",
    "\n",
    "linlass.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linlass.predict(X_test)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score :\",explained_variance_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score Uniform Average :\",explained_variance_score(y_test, y_pred, multioutput='uniform_average'))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Maximum Error (ME) :\",max_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Absolute Error (MAE) :\",mean_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Error (MSE) :\",mean_squared_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Log Error (MSLE) :\",mean_squared_log_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Median Absolute Error (MedAE) :\",median_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"R2 Score (R2) :\",r2_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score : 2.9634962457336655e-05\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score Uniform Average : 2.9634962457336655e-05\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Maximum Error (ME) : 6612478.372652433\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Absolute Error (MAE) : 227198.9448046691\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Error (MSE) : 158702470236.78464\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Log Error (MSLE) : 0.2852188207333007\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Median Absolute Error (MedAE) : 150510.01114453678\n",
      "--------------------------------------------------------------------------------------------------\n",
      "R2 Score (R2) : -0.05991461094270578\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "svr = svm.SVR(\n",
    "    kernel='rbf',\n",
    "    degree=3,\n",
    "    gamma='auto_deprecated',\n",
    "    coef0=0.0,\n",
    "    tol=0.001,\n",
    "    C=1.0,\n",
    "    epsilon=0.1,\n",
    "    shrinking=True,\n",
    "    cache_size=200,\n",
    "    verbose=False,\n",
    "    max_iter=-1,\n",
    ")\n",
    "\n",
    "svr.fit(X_train, y_train)  \n",
    "\n",
    "y_pred = svr.predict(X_test)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score :\",explained_variance_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score Uniform Average :\",explained_variance_score(y_test, y_pred, multioutput='uniform_average'))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Maximum Error (ME) :\",max_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Absolute Error (MAE) :\",mean_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Error (MSE) :\",mean_squared_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Log Error (MSLE) :\",mean_squared_log_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Median Absolute Error (MedAE) :\",median_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"R2 Score (R2) :\",r2_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformed Target Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score : 0.11963992266389734\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score Uniform Average : 0.11963992266389734\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Maximum Error (ME) : 6755529.695166179\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Absolute Error (MAE) : 178196.28431840078\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Error (MSE) : 132857992866.61635\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Log Error (MSLE) : 0.1475434631407419\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Median Absolute Error (MedAE) : 113654.11784739455\n",
      "--------------------------------------------------------------------------------------------------\n",
      "R2 Score (R2) : 0.11269101476651688\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "def func(x):\n",
    "    return np.log(x)\n",
    "def inverse_func(x):\n",
    "    return np.exp(x)\n",
    "\n",
    "regr = TransformedTargetRegressor( transformer = QuantileTransformer(output_distribution='normal'),\n",
    "                                   regressor=linreg,\n",
    "                                   #func=func,\n",
    "                                   #inverse_func=inverse_func\n",
    "                                 )\n",
    "# Either use transformer or function & inverese function\n",
    "\n",
    "regr.fit(X_train, y_train) \n",
    "\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score :\",explained_variance_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score Uniform Average :\",explained_variance_score(y_test, y_pred, multioutput='uniform_average'))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Maximum Error (ME) :\",max_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Absolute Error (MAE) :\",mean_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Error (MSE) :\",mean_squared_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Log Error (MSLE) :\",mean_squared_log_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Median Absolute Error (MedAE) :\",median_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"R2 Score (R2) :\",r2_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score : 0.5025528854309513\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score Uniform Average : 0.5025528854309513\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Maximum Error (ME) : 4191516.4445031662\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Absolute Error (MAE) : 174277.88125644292\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Error (MSE) : 74496848244.17697\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Log Error (MSLE) : 0.15819791385822252\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Median Absolute Error (MedAE) : 126295.90384339466\n",
      "--------------------------------------------------------------------------------------------------\n",
      "R2 Score (R2) : 0.5024633340276582\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "linbayes = linear_model.BayesianRidge(\n",
    "    n_iter=300,\n",
    "    tol=0.001,\n",
    "    alpha_1=1e-06,\n",
    "    alpha_2=1e-06,\n",
    "    lambda_1=1e-06,\n",
    "    lambda_2=1e-06,\n",
    "    compute_score=False,\n",
    "    fit_intercept=True,\n",
    "    normalize=False,\n",
    "    copy_X=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "linbayes.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linbayes.predict(X_test)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score :\",explained_variance_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score Uniform Average :\",explained_variance_score(y_test, y_pred, multioutput='uniform_average'))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Maximum Error (ME) :\",max_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Absolute Error (MAE) :\",mean_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Error (MSE) :\",mean_squared_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Log Error (MSLE) :\",mean_squared_log_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Median Absolute Error (MedAE) :\",median_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"R2 Score (R2) :\",r2_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Huber Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score : 0.4740211311375433\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score Uniform Average : 0.4740211311375433\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Maximum Error (ME) : 4734852.297892416\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Absolute Error (MAE) : 169708.0772659139\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Error (MSE) : 80388824246.9496\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Log Error (MSLE) : 0.15027959150365977\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Median Absolute Error (MedAE) : 118697.82961906731\n",
      "--------------------------------------------------------------------------------------------------\n",
      "R2 Score (R2) : 0.46311302370580276\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "linhub = linear_model.HuberRegressor(\n",
    "    epsilon=1.35,\n",
    "    max_iter=100,\n",
    "    alpha=0.0001,\n",
    "    warm_start=False,\n",
    "    fit_intercept=True,\n",
    "    tol=1e-05,\n",
    ")\n",
    "\n",
    "linhub.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linhub.predict(X_test)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score :\",explained_variance_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score Uniform Average :\",explained_variance_score(y_test, y_pred, multioutput='uniform_average'))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Maximum Error (ME) :\",max_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Absolute Error (MAE) :\",mean_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Error (MSE) :\",mean_squared_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Log Error (MSLE) :\",mean_squared_log_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Median Absolute Error (MedAE) :\",median_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"R2 Score (R2) :\",r2_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANdom SAmple Consensus Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score : 0.47648928003560076\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score Uniform Average : 0.47648928003560076\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Maximum Error (ME) : 4530326.122804137\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Absolute Error (MAE) : 177934.70843785504\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Error (MSE) : 80508415632.09132\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Log Error (MSLE) : 0.19154333914763352\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Median Absolute Error (MedAE) : 128197.41951631266\n",
      "--------------------------------------------------------------------------------------------------\n",
      "R2 Score (R2) : 0.4623143174458091\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from numpy import inf\n",
    "\n",
    "linran = linear_model.RANSACRegressor(\n",
    "    base_estimator=None,\n",
    "    min_samples=None,\n",
    "    residual_threshold=None,\n",
    "    is_data_valid=None,\n",
    "    is_model_valid=None,\n",
    "    max_trials=100,\n",
    "    max_skips=inf,\n",
    "    stop_n_inliers=inf,\n",
    "    stop_score=inf,\n",
    "    stop_probability=0.99,\n",
    "    loss='absolute_loss',\n",
    "    random_state=None,\n",
    ")\n",
    "\n",
    "linran.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linran.predict(X_test)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score :\",explained_variance_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score Uniform Average :\",explained_variance_score(y_test, y_pred, multioutput='uniform_average'))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Maximum Error (ME) :\",max_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Absolute Error (MAE) :\",mean_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Error (MSE) :\",mean_squared_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Log Error (MSLE) :\",mean_squared_log_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Median Absolute Error (MedAE) :\",median_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"R2 Score (R2) :\",r2_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score : 0.22132444751449243\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score Uniform Average : 0.22132444751449243\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Maximum Error (ME) : 5420000.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Absolute Error (MAE) : 203609.74647567497\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Error (MSE) : 116594331277.67424\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Log Error (MSLE) : 0.21411726704863857\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Median Absolute Error (MedAE) : 128000.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "R2 Score (R2) : 0.22130994501900814\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn import metrics\n",
    "\n",
    "dtree = DecisionTreeRegressor(\n",
    "    criterion='mse',\n",
    "    splitter='best',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=None,\n",
    "    random_state=None,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    presort=False,\n",
    ")\n",
    "\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dtree.predict(X_test)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score :\",explained_variance_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score Uniform Average :\",explained_variance_score(y_test, y_pred, multioutput='uniform_average'))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Maximum Error (ME) :\",max_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Absolute Error (MAE) :\",mean_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Error (MSE) :\",mean_squared_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Log Error (MSLE) :\",mean_squared_log_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Median Absolute Error (MedAE) :\",median_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"R2 Score (R2) :\",r2_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score : 0.428495582557755\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score Uniform Average : 0.428495582557755\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Maximum Error (ME) : 4180800.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Absolute Error (MAE) : 183164.9213040796\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Error (MSE) : 85576448650.44344\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Log Error (MSLE) : 0.17346546718976377\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Median Absolute Error (MedAE) : 122375.40000000002\n",
      "--------------------------------------------------------------------------------------------------\n",
      "R2 Score (R2) : 0.4284668150290105\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "rfr = RandomForestRegressor(\n",
    "    n_estimators='warn',\n",
    "    criterion='mse',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features='auto',\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    bootstrap=True,\n",
    "    oob_score=False,\n",
    "    n_jobs=None,\n",
    "    random_state=None,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    ")\n",
    "\n",
    "rfr.fit(X_train,y_train)\n",
    "\n",
    "y_pred = rfr.predict(X_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score :\",explained_variance_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score Uniform Average :\",explained_variance_score(y_test, y_pred, multioutput='uniform_average'))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Maximum Error (ME) :\",max_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Absolute Error (MAE) :\",mean_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Error (MSE) :\",mean_squared_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Log Error (MSLE) :\",mean_squared_log_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Median Absolute Error (MedAE) :\",median_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"R2 Score (R2) :\",r2_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score : 0.5373906994756107\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score Uniform Average : 0.5373906994756107\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Maximum Error (ME) : 4975684.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Absolute Error (MAE) : 165979.19164446936\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Error (MSE) : 69268602953.5168\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Log Error (MSLE) : 0.14214645474233079\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Median Absolute Error (MedAE) : 117154.5625\n",
      "--------------------------------------------------------------------------------------------------\n",
      "R2 Score (R2) : 0.5373808344603559\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    silent=True,\n",
    "    objective='reg:linear',\n",
    "    booster='gbtree',\n",
    "    n_jobs=1,\n",
    "    nthread=None,\n",
    "    gamma=0,\n",
    "    min_child_weight=1,\n",
    "    max_delta_step=0,\n",
    "    subsample=1,\n",
    "    colsample_bytree=1,\n",
    "    colsample_bylevel=1,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=1,\n",
    "    base_score=0.5,\n",
    "    random_state=0,\n",
    "    seed=None,\n",
    "    missing=None,\n",
    "    importance_type='gain',\n",
    ")\n",
    "\n",
    "xgb.fit(X_train,y_train)\n",
    "\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score :\",explained_variance_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score Uniform Average :\",explained_variance_score(y_test, y_pred, multioutput='uniform_average'))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Maximum Error (ME) :\",max_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Absolute Error (MAE) :\",mean_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Error (MSE) :\",mean_squared_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Log Error (MSLE) :\",mean_squared_log_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Median Absolute Error (MedAE) :\",median_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"R2 Score (R2) :\",r2_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "logreg = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    dual=False,\n",
    "    tol=0.0001,\n",
    "    C=1.0,\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1,\n",
    "    class_weight=None,\n",
    "    random_state=None,\n",
    "    solver='lbfgs',\n",
    "    max_iter=100,\n",
    "    multi_class='warn',\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    n_jobs=None,\n",
    "    l1_ratio=None,\n",
    ")\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score :\",explained_variance_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score Uniform Average :\",explained_variance_score(y_test, y_pred, multioutput='uniform_average'))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Maximum Error (ME) :\",max_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Absolute Error (MAE) :\",mean_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Error (MSE) :\",mean_squared_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Log Error (MSLE) :\",mean_squared_log_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Median Absolute Error (MedAE) :\",median_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"R2 Score (R2) :\",r2_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer perceptron (MLP) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score : 0.4841915115522234\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score Uniform Average : 0.4841915115522234\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Maximum Error (ME) : 4437193.144158075\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Absolute Error (MAE) : 176345.94007722626\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Error (MSE) : 77238856838.0112\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Log Error (MSLE) : 0.16132653556787718\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Median Absolute Error (MedAE) : 128498.47771539603\n",
      "--------------------------------------------------------------------------------------------------\n",
      "R2 Score (R2) : 0.48415048125605886\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlpreg = MLPRegressor(\n",
    "    hidden_layer_sizes=(100,),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    batch_size='auto',\n",
    "    learning_rate='constant',\n",
    "    learning_rate_init=0.001,\n",
    "    power_t=0.5,\n",
    "    max_iter=200,\n",
    "    shuffle=True,\n",
    "    random_state=None,\n",
    "    tol=0.0001,\n",
    "    verbose=False,\n",
    "    warm_start=False,\n",
    "    momentum=0.9,\n",
    "    nesterovs_momentum=True,\n",
    "    early_stopping=False,\n",
    "    validation_fraction=0.1,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-08,\n",
    "    n_iter_no_change=10,\n",
    ")\n",
    "\n",
    "mlpreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mlpreg.predict(X_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score :\",explained_variance_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score Uniform Average :\",explained_variance_score(y_test, y_pred, multioutput='uniform_average'))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Maximum Error (ME) :\",max_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Absolute Error (MAE) :\",mean_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Error (MSE) :\",mean_squared_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Log Error (MSLE) :\",mean_squared_log_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Median Absolute Error (MedAE) :\",median_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"R2 Score (R2) :\",r2_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Star Boost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score : 0.5214996870608759\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Explained Variance Score Uniform Average : 0.5214996870608759\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Maximum Error (ME) : 4422148.5\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Absolute Error (MAE) : 169570.65107160382\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Error (MSE) : 71651486296.111\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Mean Squared Log Error (MSLE) : 0.15013434762528813\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Median Absolute Error (MedAE) : 125470.375\n",
      "--------------------------------------------------------------------------------------------------\n",
      "R2 Score (R2) : 0.5214664453067448\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import tree\n",
    "import starboost as sb\n",
    "\n",
    "\n",
    "strboost = sb.BoostingRegressor(\n",
    "    loss=None,\n",
    "    base_estimator=tree.DecisionTreeRegressor(max_depth=3),\n",
    "    tree_flavor=False,\n",
    "    n_estimators=30,\n",
    "    init_estimator=None,\n",
    "    line_searcher=None,\n",
    "    learning_rate=0.1,\n",
    "    row_sampling=1.0,\n",
    "    col_sampling=1.0,\n",
    "    eval_metric=None,\n",
    "    early_stopping_rounds=None,\n",
    "    random_state=None,\n",
    ")\n",
    "\n",
    "strboost.fit(X_train, y_train)\n",
    "\n",
    "y_pred = strboost.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score :\",explained_variance_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Explained Variance Score Uniform Average :\",explained_variance_score(y_test, y_pred, multioutput='uniform_average'))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Maximum Error (ME) :\",max_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Absolute Error (MAE) :\",mean_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Error (MSE) :\",mean_squared_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Mean Squared Log Error (MSLE) :\",mean_squared_log_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Median Absolute Error (MedAE) :\",median_absolute_error(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"R2 Score (R2) :\",r2_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Accuracy Score : 0.984999299032665\n",
      "--------------------------------------------------------------------------------------------------\n",
      "AUC Metric : nan\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Average Precision Score : 0.009874552808802074\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Balanced Accuracy Score : 0.5126198334119126\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Brier Score Loss : 0.015000700967334923\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Classification Report :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      7070\n",
      "           1       0.04      0.03      0.04        63\n",
      "\n",
      "    accuracy                           0.98      7133\n",
      "   macro avg       0.52      0.51      0.51      7133\n",
      "weighted avg       0.98      0.98      0.98      7133\n",
      "\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Cohen Kappa Score : 0.028615973985478105\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Confusion Matrix : [[7024   46]\n",
      " [  61    2]]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "F1 Score : 0.03603603603603604\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Hamming Loss : 0.015000700967334923\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Hinge Loss : 1.0061685125473152\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Jaccard Score : 0.01834862385321101\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Log Loss : 0.518111013001155\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Matthews Corr Coef : 0.028885045325034903\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Multilabel Confusion Matrix : [[[   2   61]\n",
      "  [  46 7024]]\n",
      "\n",
      " [[7024   46]\n",
      "  [  61    2]]]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Recall Curve : (array([0.00883219, 0.04166667, 1.        ]), array([1.        , 0.03174603, 0.        ]), array([0, 1]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Recall FScore Support : (array([0.99139026, 0.04166667]), array([0.99349364, 0.03174603]), array([0.99244083, 0.03603604]), array([7070,   63]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Score : 0.041666666666666664\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Recall Score : 0.031746031746031744\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ROC AUC Score : 0.5126198334119126\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ROC Curve : (array([0.        , 0.00650636, 1.        ]), array([0.        , 0.03174603, 1.        ]), array([2, 1, 0]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Zero One Loss : 0.015000700967334968\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import hinge_loss\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dtree = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    splitter='best',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=None,\n",
    "    random_state=None,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    class_weight=None,\n",
    "    presort=False,\n",
    ")\n",
    "\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dtree.predict(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=2)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Accuracy Score :\",accuracy_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"AUC Metric :\",metrics.auc(fpr, tpr))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Average Precision Score :\",average_precision_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Balanced Accuracy Score :\",balanced_accuracy_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Brier Score Loss :\",brier_score_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Classification Report :\",classification_report(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Cohen Kappa Score :\",cohen_kappa_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Confusion Matrix :\",confusion_matrix(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"F1 Score :\",f1_score(y_test, y_pred))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"FBeta Score :\",fbeta_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Hamming Loss :\",hamming_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Hinge Loss :\",hinge_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Jaccard Score :\",jaccard_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Log Loss :\",log_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Matthews Corr Coef :\",matthews_corrcoef(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Multilabel Confusion Matrix :\",multilabel_confusion_matrix(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Recall Curve :\",precision_recall_curve(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Recall FScore Support :\",precision_recall_fscore_support(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Score :\",precision_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Recall Score :\",recall_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"ROC AUC Score :\",roc_auc_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"ROC Curve :\",roc_curve(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Zero One Loss :\",zero_one_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Accuracy Score : 0.9894854899761671\n",
      "--------------------------------------------------------------------------------------------------\n",
      "AUC Metric : nan\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Average Precision Score : 0.01252005545430472\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Balanced Accuracy Score : 0.5148829168631148\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Brier Score Loss : 0.01051451002383289\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Classification Report :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      7070\n",
      "           1       0.12      0.03      0.05        63\n",
      "\n",
      "    accuracy                           0.99      7133\n",
      "   macro avg       0.56      0.51      0.52      7133\n",
      "weighted avg       0.98      0.99      0.99      7133\n",
      "\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Cohen Kappa Score : 0.047224265393390086\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Confusion Matrix : [[7056   14]\n",
      " [  61    2]]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "F1 Score : 0.05063291139240506\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Hamming Loss : 0.01051451002383289\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Hinge Loss : 1.0016823216038133\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Jaccard Score : 0.025974025974025976\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Log Loss : 0.3631598799919152\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Matthews Corr Coef : 0.05886939313188418\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Multilabel Confusion Matrix : [[[   2   61]\n",
      "  [  14 7056]]\n",
      "\n",
      " [[7056   14]\n",
      "  [  61    2]]]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Recall Curve : (array([0.00883219, 0.125     , 1.        ]), array([1.        , 0.03174603, 0.        ]), array([0, 1]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Recall FScore Support : (array([0.99142897, 0.125     ]), array([0.9980198 , 0.03174603]), array([0.99471347, 0.05063291]), array([7070,   63]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Score : 0.125\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Recall Score : 0.031746031746031744\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ROC AUC Score : 0.5148829168631148\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ROC Curve : (array([0.       , 0.0019802, 1.       ]), array([0.        , 0.03174603, 1.        ]), array([2, 1, 0]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Zero One Loss : 0.010514510023832857\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "rfc = RandomForestClassifier(\n",
    "    n_estimators='warn',\n",
    "    criterion='gini',\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features='auto',\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    bootstrap=True,\n",
    "    oob_score=False,\n",
    "    n_jobs=None,\n",
    "    random_state=None,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    class_weight=None,\n",
    ")\n",
    "\n",
    "rfc.fit(X_train,y_train)\n",
    "\n",
    "y_pred = rfc.predict(X_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=2)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Accuracy Score :\",accuracy_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"AUC Metric :\",metrics.auc(fpr, tpr))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Average Precision Score :\",average_precision_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Balanced Accuracy Score :\",balanced_accuracy_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Brier Score Loss :\",brier_score_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Classification Report :\",classification_report(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Cohen Kappa Score :\",cohen_kappa_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Confusion Matrix :\",confusion_matrix(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"F1 Score :\",f1_score(y_test, y_pred))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"FBeta Score :\",fbeta_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Hamming Loss :\",hamming_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Hinge Loss :\",hinge_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Jaccard Score :\",jaccard_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Log Loss :\",log_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Matthews Corr Coef :\",matthews_corrcoef(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Multilabel Confusion Matrix :\",multilabel_confusion_matrix(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Recall Curve :\",precision_recall_curve(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Recall FScore Support :\",precision_recall_fscore_support(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Score :\",precision_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Recall Score :\",recall_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"ROC AUC Score :\",roc_auc_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"ROC Curve :\",roc_curve(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Zero One Loss :\",zero_one_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Accuracy Score : 0.9911678115799804\n",
      "--------------------------------------------------------------------------------------------------\n",
      "AUC Metric : nan\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Average Precision Score : 0.008832188420019628\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Balanced Accuracy Score : 0.5\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Brier Score Loss : 0.008832188420019628\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Classification Report :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      7070\n",
      "           1       0.00      0.00      0.00        63\n",
      "\n",
      "    accuracy                           0.99      7133\n",
      "   macro avg       0.50      0.50      0.50      7133\n",
      "weighted avg       0.98      0.99      0.99      7133\n",
      "\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Cohen Kappa Score : 0.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Confusion Matrix : [[7070    0]\n",
      " [  63    0]]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "F1 Score : 0.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Hamming Loss : 0.008832188420019628\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Hinge Loss : 1.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Jaccard Score : 0.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Log Loss : 0.30505298091677835\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Matthews Corr Coef : 0.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Multilabel Confusion Matrix : [[[   0   63]\n",
      "  [   0 7070]]\n",
      "\n",
      " [[7070    0]\n",
      "  [  63    0]]]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Recall Curve : (array([0.00883219, 1.        ]), array([1., 0.]), array([0]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Recall FScore Support : (array([0.99116781, 0.        ]), array([1., 0.]), array([0.99556432, 0.        ]), array([7070,   63]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Score : 0.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Recall Score : 0.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ROC AUC Score : 0.5\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ROC Curve : (array([0., 1.]), array([0., 1.]), array([1, 0]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Zero One Loss : 0.008832188420019649\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/classification.py:872: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgc = XGBClassifier(\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    silent=True,\n",
    "    objective='binary:logistic',\n",
    "    booster='gbtree',\n",
    "    n_jobs=1,\n",
    "    nthread=None,\n",
    "    gamma=0,\n",
    "    min_child_weight=1,\n",
    "    max_delta_step=0,\n",
    "    subsample=1,\n",
    "    colsample_bytree=1,\n",
    "    colsample_bylevel=1,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=1,\n",
    "    base_score=0.5,\n",
    "    random_state=0,\n",
    "    seed=None,\n",
    "    missing=None\n",
    ")\n",
    "\n",
    "xgc.fit(X_train,y_train)\n",
    "\n",
    "y_pred = xgc.predict(X_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=2)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Accuracy Score :\",accuracy_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"AUC Metric :\",metrics.auc(fpr, tpr))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Average Precision Score :\",average_precision_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Balanced Accuracy Score :\",balanced_accuracy_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Brier Score Loss :\",brier_score_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Classification Report :\",classification_report(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Cohen Kappa Score :\",cohen_kappa_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Confusion Matrix :\",confusion_matrix(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"F1 Score :\",f1_score(y_test, y_pred))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"FBeta Score :\",fbeta_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Hamming Loss :\",hamming_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Hinge Loss :\",hinge_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Jaccard Score :\",jaccard_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Log Loss :\",log_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Matthews Corr Coef :\",matthews_corrcoef(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Multilabel Confusion Matrix :\",multilabel_confusion_matrix(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Recall Curve :\",precision_recall_curve(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Recall FScore Support :\",precision_recall_fscore_support(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Score :\",precision_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Recall Score :\",recall_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"ROC AUC Score :\",roc_auc_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"ROC Curve :\",roc_curve(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Zero One Loss :\",zero_one_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Accuracy Score : 0.9798121407542408\n",
      "--------------------------------------------------------------------------------------------------\n",
      "AUC Metric : nan\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Average Precision Score : 0.025937001260228867\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Balanced Accuracy Score : 0.5807952223793807\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Brier Score Loss : 0.020187859245759148\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Classification Report :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      7070\n",
      "           1       0.11      0.17      0.13        63\n",
      "\n",
      "    accuracy                           0.98      7133\n",
      "   macro avg       0.55      0.58      0.56      7133\n",
      "weighted avg       0.98      0.98      0.98      7133\n",
      "\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Cohen Kappa Score : 0.12291691572026298\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Confusion Matrix : [[6978   92]\n",
      " [  52   11]]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "F1 Score : 0.1325301204819277\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Hamming Loss : 0.020187859245759148\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Hinge Loss : 1.0113556708257396\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Jaccard Score : 0.07096774193548387\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Log Loss : 0.697274269428112\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Matthews Corr Coef : 0.12673582114682663\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Multilabel Confusion Matrix : [[[  11   52]\n",
      "  [  92 6978]]\n",
      "\n",
      " [[6978   92]\n",
      "  [  52   11]]]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Recall Curve : (array([0.00883219, 0.10679612, 1.        ]), array([1.        , 0.17460317, 0.        ]), array([0, 1]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Recall FScore Support : (array([0.99260313, 0.10679612]), array([0.98698727, 0.17460317]), array([0.98978723, 0.13253012]), array([7070,   63]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Score : 0.10679611650485436\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Recall Score : 0.1746031746031746\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ROC AUC Score : 0.5807952223793809\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ROC Curve : (array([0.        , 0.01301273, 1.        ]), array([0.        , 0.17460317, 1.        ]), array([2, 1, 0]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Zero One Loss : 0.020187859245759165\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "\n",
    "naive = GaussianNB(\n",
    "    priors=None,\n",
    "    var_smoothing=1e-09)\n",
    "\n",
    "naive.fit(X_train, y_train)\n",
    "\n",
    "y_pred = naive.predict(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=2)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Accuracy Score :\",accuracy_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"AUC Metric :\",metrics.auc(fpr, tpr))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Average Precision Score :\",average_precision_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Balanced Accuracy Score :\",balanced_accuracy_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Brier Score Loss :\",brier_score_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Classification Report :\",classification_report(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Cohen Kappa Score :\",cohen_kappa_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Confusion Matrix :\",confusion_matrix(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"F1 Score :\",f1_score(y_test, y_pred))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"FBeta Score :\",fbeta_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Hamming Loss :\",hamming_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Hinge Loss :\",hinge_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Jaccard Score :\",jaccard_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Log Loss :\",log_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Matthews Corr Coef :\",matthews_corrcoef(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Multilabel Confusion Matrix :\",multilabel_confusion_matrix(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Recall Curve :\",precision_recall_curve(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Recall FScore Support :\",precision_recall_fscore_support(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Score :\",precision_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Recall Score :\",recall_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"ROC AUC Score :\",roc_auc_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"ROC Curve :\",roc_curve(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Zero One Loss :\",zero_one_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stocastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Accuracy Score : 0.9911678115799804\n",
      "--------------------------------------------------------------------------------------------------\n",
      "AUC Metric : nan\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Average Precision Score : 0.008832188420019628\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Balanced Accuracy Score : 0.5\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Brier Score Loss : 0.008832188420019628\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Classification Report :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      7070\n",
      "           1       0.00      0.00      0.00        63\n",
      "\n",
      "    accuracy                           0.99      7133\n",
      "   macro avg       0.50      0.50      0.50      7133\n",
      "weighted avg       0.98      0.99      0.99      7133\n",
      "\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Cohen Kappa Score : 0.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Confusion Matrix : [[7070    0]\n",
      " [  63    0]]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "F1 Score : 0.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Hamming Loss : 0.008832188420019628\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Hinge Loss : 1.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Jaccard Score : 0.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Log Loss : 0.30505298091677835\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Matthews Corr Coef : 0.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Multilabel Confusion Matrix : [[[   0   63]\n",
      "  [   0 7070]]\n",
      "\n",
      " [[7070    0]\n",
      "  [  63    0]]]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Recall Curve : (array([0.00883219, 1.        ]), array([1., 0.]), array([0]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Recall FScore Support : (array([0.99116781, 0.        ]), array([1., 0.]), array([0.99556432, 0.        ]), array([7070,   63]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Score : 0.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Recall Score : 0.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ROC AUC Score : 0.5\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ROC Curve : (array([0., 1.]), array([0., 1.]), array([1, 0]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Zero One Loss : 0.008832188420019649\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/classification.py:872: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "sgd = SGDClassifier(\n",
    "    loss='hinge',\n",
    "    penalty='l2',\n",
    "    alpha=0.0001,\n",
    "    l1_ratio=0.15,\n",
    "    fit_intercept=True,\n",
    "    max_iter=1000,\n",
    "    tol=0.001,\n",
    "    shuffle=True,\n",
    "    verbose=0,\n",
    "    epsilon=0.1,\n",
    "    n_jobs=None,\n",
    "    random_state=None,\n",
    "    learning_rate='optimal',\n",
    "    eta0=0.0,\n",
    "    power_t=0.5,\n",
    "    early_stopping=False,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=5,\n",
    "    class_weight=None,\n",
    "    warm_start=False,\n",
    "    average=False,\n",
    ")\n",
    "\n",
    "sgd.fit(X_train, y_train)  \n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=2)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Accuracy Score :\",accuracy_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"AUC Metric :\",metrics.auc(fpr, tpr))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Average Precision Score :\",average_precision_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Balanced Accuracy Score :\",balanced_accuracy_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Brier Score Loss :\",brier_score_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Classification Report :\",classification_report(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Cohen Kappa Score :\",cohen_kappa_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Confusion Matrix :\",confusion_matrix(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"F1 Score :\",f1_score(y_test, y_pred))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"FBeta Score :\",fbeta_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Hamming Loss :\",hamming_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Hinge Loss :\",hinge_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Jaccard Score :\",jaccard_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Log Loss :\",log_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Matthews Corr Coef :\",matthews_corrcoef(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Multilabel Confusion Matrix :\",multilabel_confusion_matrix(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Recall Curve :\",precision_recall_curve(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Recall FScore Support :\",precision_recall_fscore_support(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Score :\",precision_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Recall Score :\",recall_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"ROC AUC Score :\",roc_auc_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"ROC Curve :\",roc_curve(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Zero One Loss :\",zero_one_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Accuracy Score : 0.9900462638441049\n",
      "--------------------------------------------------------------------------------------------------\n",
      "AUC Metric : nan\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Average Precision Score : 0.01861568965171937\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Balanced Accuracy Score : 0.523031588873173\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Brier Score Loss : 0.009953736155895135\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Classification Report :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      7070\n",
      "           1       0.21      0.05      0.08        63\n",
      "\n",
      "    accuracy                           0.99      7133\n",
      "   macro avg       0.60      0.52      0.54      7133\n",
      "weighted avg       0.98      0.99      0.99      7133\n",
      "\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Cohen Kappa Score : 0.07495109383590548\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Confusion Matrix : [[7059   11]\n",
      " [  60    3]]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "F1 Score : 0.07792207792207793\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Hamming Loss : 0.009953736155895135\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Hinge Loss : 1.0011215477358755\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Jaccard Score : 0.04054054054054054\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Log Loss : 0.3437911004640961\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Matthews Corr Coef : 0.09737789057661986\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Multilabel Confusion Matrix : [[[   3   60]\n",
      "  [  11 7059]]\n",
      "\n",
      " [[7059   11]\n",
      "  [  60    3]]]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Recall Curve : (array([0.00883219, 0.21428571, 1.        ]), array([1.        , 0.04761905, 0.        ]), array([0, 1]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Recall FScore Support : (array([0.99157185, 0.21428571]), array([0.99844413, 0.04761905]), array([0.99499612, 0.07792208]), array([7070,   63]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Score : 0.21428571428571427\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Recall Score : 0.047619047619047616\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ROC AUC Score : 0.523031588873173\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ROC Curve : (array([0.        , 0.00155587, 1.        ]), array([0.        , 0.04761905, 1.        ]), array([2, 1, 0]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Zero One Loss : 0.009953736155895121\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "bgclass = BaggingClassifier(\n",
    "    base_estimator=None,\n",
    "    n_estimators=10,\n",
    "    max_samples=1.0,\n",
    "    max_features=1.0,\n",
    "    bootstrap=True,\n",
    "    bootstrap_features=False,\n",
    "    oob_score=False,\n",
    "    warm_start=False,\n",
    "    n_jobs=None,\n",
    "    random_state=None,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "bgclass.fit(X_train,y_train)\n",
    "\n",
    "y_pred = bgclass.predict(X_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=2)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Accuracy Score :\",accuracy_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"AUC Metric :\",metrics.auc(fpr, tpr))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Average Precision Score :\",average_precision_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Balanced Accuracy Score :\",balanced_accuracy_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Brier Score Loss :\",brier_score_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Classification Report :\",classification_report(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Cohen Kappa Score :\",cohen_kappa_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Confusion Matrix :\",confusion_matrix(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"F1 Score :\",f1_score(y_test, y_pred))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"FBeta Score :\",fbeta_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Hamming Loss :\",hamming_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Hinge Loss :\",hinge_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Jaccard Score :\",jaccard_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Log Loss :\",log_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Matthews Corr Coef :\",matthews_corrcoef(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Multilabel Confusion Matrix :\",multilabel_confusion_matrix(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Recall Curve :\",precision_recall_curve(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Recall FScore Support :\",precision_recall_fscore_support(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Score :\",precision_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Recall Score :\",recall_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"ROC AUC Score :\",roc_auc_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"ROC Curve :\",roc_curve(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Zero One Loss :\",zero_one_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Accuracy Score : 0.9908874246460115\n",
      "--------------------------------------------------------------------------------------------------\n",
      "AUC Metric : nan\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Average Precision Score : 0.012660248921289158\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Balanced Accuracy Score : 0.5077243438629577\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Brier Score Loss : 0.009112575353988505\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Classification Report :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      7070\n",
      "           1       0.25      0.02      0.03        63\n",
      "\n",
      "    accuracy                           0.99      7133\n",
      "   macro avg       0.62      0.51      0.51      7133\n",
      "weighted avg       0.98      0.99      0.99      7133\n",
      "\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Cohen Kappa Score : 0.028826556795354952\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Confusion Matrix : [[7067    3]\n",
      " [  62    1]]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "F1 Score : 0.029850746268656716\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Hamming Loss : 0.009112575353988505\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Hinge Loss : 1.000280386933969\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Jaccard Score : 0.015151515151515152\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Log Loss : 0.3147375388281917\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Matthews Corr Coef : 0.061055848801045344\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Multilabel Confusion Matrix : [[[   1   62]\n",
      "  [   3 7067]]\n",
      "\n",
      " [[7067    3]\n",
      "  [  62    1]]]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Recall Curve : (array([0.00883219, 0.25      , 1.        ]), array([1.        , 0.01587302, 0.        ]), array([0, 1]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Recall FScore Support : (array([0.99130313, 0.25      ]), array([0.99957567, 0.01587302]), array([0.99542221, 0.02985075]), array([7070,   63]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Score : 0.25\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Recall Score : 0.015873015873015872\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ROC AUC Score : 0.5077243438629576\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ROC Curve : (array([0.00000000e+00, 4.24328147e-04, 1.00000000e+00]), array([0.        , 0.01587302, 1.        ]), array([2, 1, 0]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Zero One Loss : 0.009112575353988461\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "abclass = AdaBoostClassifier(\n",
    "    base_estimator=None,\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    algorithm='SAMME.R',\n",
    "    random_state=None,\n",
    ")\n",
    "\n",
    "abclass.fit(X_train,y_train)\n",
    "\n",
    "y_pred = abclass.predict(X_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=2)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Accuracy Score :\",accuracy_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"AUC Metric :\",metrics.auc(fpr, tpr))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Average Precision Score :\",average_precision_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Balanced Accuracy Score :\",balanced_accuracy_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Brier Score Loss :\",brier_score_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Classification Report :\",classification_report(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Cohen Kappa Score :\",cohen_kappa_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Confusion Matrix :\",confusion_matrix(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"F1 Score :\",f1_score(y_test, y_pred))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"FBeta Score :\",fbeta_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Hamming Loss :\",hamming_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Hinge Loss :\",hinge_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Jaccard Score :\",jaccard_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Log Loss :\",log_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Matthews Corr Coef :\",matthews_corrcoef(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Multilabel Confusion Matrix :\",multilabel_confusion_matrix(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Recall Curve :\",precision_recall_curve(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Recall FScore Support :\",precision_recall_fscore_support(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Score :\",precision_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Recall Score :\",recall_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"ROC AUC Score :\",roc_auc_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"ROC Curve :\",roc_curve(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Zero One Loss :\",zero_one_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "vclass = VotingClassifier(\n",
    "    estimators=[('rfr', rfr), ('xgb', xgb), ('naive', naive)],\n",
    "    voting='hard',\n",
    "    weights=None,\n",
    "    n_jobs=None,\n",
    "    flatten_transform=True,\n",
    ")\n",
    "\n",
    "vclass.fit(X_train,y_train)\n",
    "\n",
    "y_pred = vclass.predict(X_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=2)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Accuracy Score :\",accuracy_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"AUC Metric :\",metrics.auc(fpr, tpr))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Average Precision Score :\",average_precision_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Balanced Accuracy Score :\",balanced_accuracy_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Brier Score Loss :\",brier_score_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Classification Report :\",classification_report(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Cohen Kappa Score :\",cohen_kappa_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Confusion Matrix :\",confusion_matrix(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"F1 Score :\",f1_score(y_test, y_pred))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"FBeta Score :\",fbeta_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Hamming Loss :\",hamming_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Hinge Loss :\",hinge_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Jaccard Score :\",jaccard_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Log Loss :\",log_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Matthews Corr Coef :\",matthews_corrcoef(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Multilabel Confusion Matrix :\",multilabel_confusion_matrix(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Recall Curve :\",precision_recall_curve(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Recall FScore Support :\",precision_recall_fscore_support(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Score :\",precision_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Recall Score :\",recall_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"ROC AUC Score :\",roc_auc_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"ROC Curve :\",roc_curve(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Zero One Loss :\",zero_one_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer perceptron (MLP) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Accuracy Score : 0.9911678115799804\n",
      "--------------------------------------------------------------------------------------------------\n",
      "AUC Metric : nan\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Average Precision Score : 0.008832188420019628\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Balanced Accuracy Score : 0.5\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Brier Score Loss : 0.008832188420019628\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Classification Report :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      7070\n",
      "           1       0.00      0.00      0.00        63\n",
      "\n",
      "    accuracy                           0.99      7133\n",
      "   macro avg       0.50      0.50      0.50      7133\n",
      "weighted avg       0.98      0.99      0.99      7133\n",
      "\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Cohen Kappa Score : 0.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Confusion Matrix : [[7070    0]\n",
      " [  63    0]]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "F1 Score : 0.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Hamming Loss : 0.008832188420019628\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Hinge Loss : 1.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Jaccard Score : 0.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Log Loss : 0.30505298091677835\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Matthews Corr Coef : 0.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Multilabel Confusion Matrix : [[[   0   63]\n",
      "  [   0 7070]]\n",
      "\n",
      " [[7070    0]\n",
      "  [  63    0]]]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Recall Curve : (array([0.00883219, 1.        ]), array([1., 0.]), array([0]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Recall FScore Support : (array([0.99116781, 0.        ]), array([1., 0.]), array([0.99556432, 0.        ]), array([7070,   63]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Precision Score : 0.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Recall Score : 0.0\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ROC AUC Score : 0.5\n",
      "--------------------------------------------------------------------------------------------------\n",
      "ROC Curve : (array([0., 1.]), array([0., 1.]), array([1, 0]))\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Zero One Loss : 0.008832188420019649\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/classification.py:872: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    batch_size='auto',\n",
    "    learning_rate='constant',\n",
    "    learning_rate_init=0.001,\n",
    "    power_t=0.5,\n",
    "    max_iter=200,\n",
    "    shuffle=True,\n",
    "    random_state=None,\n",
    "    tol=0.0001,\n",
    "    verbose=False,\n",
    "    warm_start=False,\n",
    "    momentum=0.9,\n",
    "    nesterovs_momentum=True,\n",
    "    early_stopping=False,\n",
    "    validation_fraction=0.1,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-08,\n",
    "    n_iter_no_change=10,\n",
    ")\n",
    "\n",
    "mlp.fit(X_train, y_train)                         \n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=2)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Accuracy Score :\",accuracy_score(y_test, y_pred)) \n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"AUC Metric :\",metrics.auc(fpr, tpr))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Average Precision Score :\",average_precision_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Balanced Accuracy Score :\",balanced_accuracy_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Brier Score Loss :\",brier_score_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Classification Report :\",classification_report(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Cohen Kappa Score :\",cohen_kappa_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Confusion Matrix :\",confusion_matrix(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"F1 Score :\",f1_score(y_test, y_pred))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"FBeta Score :\",fbeta_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Hamming Loss :\",hamming_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Hinge Loss :\",hinge_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Jaccard Score :\",jaccard_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Log Loss :\",log_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Matthews Corr Coef :\",matthews_corrcoef(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Multilabel Confusion Matrix :\",multilabel_confusion_matrix(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Recall Curve :\",precision_recall_curve(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Recall FScore Support :\",precision_recall_fscore_support(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Precision Score :\",precision_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Recall Score :\",recall_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"ROC AUC Score :\",roc_auc_score(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"ROC Curve :\",roc_curve(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Zero One Loss :\",zero_one_loss(y_test, y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiOutput Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "X, y = make_classification(n_samples=10, n_features=100, n_informative=30, n_classes=3, random_state=1)\n",
    "y2 = shuffle(y1, random_state=1)\n",
    "y3 = shuffle(y1, random_state=2)\n",
    "Y = np.vstack((y1, y2, y3)).T\n",
    "n_samples, n_features = X.shape\n",
    "n_outputs = Y.shape[1] \n",
    "n_classes = 3\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "multi_target_forest = MultiOutputClassifier(forest, n_jobs=-1)\n",
    "multi_target_forest.fit(X, Y).predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiOutput Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "X, y = make_regression(n_samples=10, n_targets=3, random_state=1)\n",
    "MultiOutputRegressor(GradientBoostingRegressor(random_state=0)).fit(X, y).predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(\n",
    "    estimator,\n",
    "    threshold=None,\n",
    "    prefit=False,\n",
    "    norm_order=1,\n",
    "    max_features=None,\n",
    ")\n",
    "X_new = model.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi - Supervised Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.semi_supervised import LabelPropagation\n",
    "\n",
    "label_prop_model = LabelPropagation(\n",
    "    kernel='rbf',\n",
    "    gamma=20,\n",
    "    n_neighbors=7,\n",
    "    max_iter=1,\n",
    "    tol=0.001,\n",
    "    n_jobs=None,\n",
    ")\n",
    "\n",
    "label_prop_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = label_prop_model.predict(X_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"R2 Score :\",metrics.r2_score(y_test,y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Spreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.semi_supervised import LabelSpreading\n",
    "\n",
    "label_prop_model = LabelSpreading(\n",
    "    kernel='rbf',\n",
    "    gamma=20,\n",
    "    n_neighbors=7,\n",
    "    max_iter=1,\n",
    "    tol=0.001,\n",
    "    n_jobs=None,\n",
    ")\n",
    "\n",
    "label_prop_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = label_prop_model.predict(X_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"R2 Score :\",metrics.r2_score(y_test,y_pred))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 0 ... 0 3 2]\n",
      "[[3.46813187e+00 2.01567070e+03 1.47802198e+00 2.17861722e+00]\n",
      " [4.33989501e+00 4.46663780e+03 1.90879265e+00 3.49606299e+00]\n",
      " [3.97142857e+00 2.95975111e+03 1.72730159e+00 2.64920635e+00]\n",
      " [2.74295223e+00 1.23820889e+03 1.31959671e+00 1.50650940e+00]]\n",
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Calinski Harabasz Score : 32526.736957525514\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Davies Bouldin Score : 0.5566293464169927\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Silhouette Score : 0.5422620112860752\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Silhouette Samples : [0.35219067 0.71658154 0.38709775 ... 0.73079125 0.47518937 0.70869416]\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.metrics.cluster import calinski_harabasz_score\n",
    "from sklearn.metrics.cluster import davies_bouldin_score\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.metrics.cluster import fowlkes_mallows_score\n",
    "from sklearn.metrics.cluster import homogeneity_completeness_v_measure\n",
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "from sklearn.metrics.cluster import mutual_info_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.metrics.cluster import silhouette_score\n",
    "from sklearn.metrics.cluster import silhouette_samples\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=4,\n",
    "    init='k-means++',\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    tol=0.0001,\n",
    "    precompute_distances='auto',\n",
    "    verbose=0,\n",
    "    random_state=None,\n",
    "    copy_x=True,\n",
    "    n_jobs=None,\n",
    "    algorithm='auto',\n",
    ").fit(X_train)\n",
    "\n",
    "print(kmeans.labels_)\n",
    "print(kmeans.cluster_centers_)\n",
    "\n",
    "kmeans.predict(X_test)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Adjusted Mutual Info Score :\",adjusted_mutual_info_score(X_test)) \n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Adjusted Rand Score :\",adjusted_rand_score(X_test))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Calinski Harabasz Score :\",calinski_harabasz_score(X_train,kmeans.labels_))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Davies Bouldin Score :\",davies_bouldin_score(X_train,kmeans.labels_))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Completeness Score :\",completeness_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Contingency Matrix :\",contingency_matrix(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Fowlkes Mallows Score :\",fowlkes_mallows_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Homogeneity Completeness V Measure :\",homogeneity_completeness_v_measure(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Homogeneity Score :\",homogeneity_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Mutual Info Score :\",mutual_info_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Normalized Mutual Info Score :\",normalized_mutual_info_score(X_test))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Silhouette Score :\",silhouette_score(X_train,kmeans.labels_))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Silhouette Samples :\",silhouette_samples(X_train,kmeans.labels_))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"V Measure Score :\",v_measure_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "spec = SpectralClustering(\n",
    "    n_clusters=4,\n",
    "    eigen_solver=None,\n",
    "    random_state=None,\n",
    "    n_init=10,\n",
    "    gamma=1.0,\n",
    "    affinity='rbf',\n",
    "    n_neighbors=10,\n",
    "    eigen_tol=0.0,\n",
    "    assign_labels='kmeans',\n",
    "    degree=3,\n",
    "    coef0=1,\n",
    "    kernel_params=None,\n",
    "    n_jobs=None,\n",
    ").fit(X_train)\n",
    "\n",
    "print(spec.labels_)\n",
    "print(spec.cluster_centers_)\n",
    "\n",
    "spec.predict(X_test)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Adjusted Mutual Info Score :\",adjusted_mutual_info_score(X_test)) \n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Adjusted Rand Score :\",adjusted_rand_score(X_test))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Calinski Harabasz Score :\",calinski_harabasz_score(X_train,spec.labels_))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Davies Bouldin Score :\",davies_bouldin_score(X_train,spec.labels_))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Completeness Score :\",completeness_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Contingency Matrix :\",contingency_matrix(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Fowlkes Mallows Score :\",fowlkes_mallows_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Homogeneity Completeness V Measure :\",homogeneity_completeness_v_measure(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Homogeneity Score :\",homogeneity_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Mutual Info Score :\",mutual_info_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Normalized Mutual Info Score :\",normalized_mutual_info_score(X_test))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Silhouette Score :\",silhouette_score(X_train,spec.labels_))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Silhouette Samples :\",silhouette_samples(X_train,spec.labels_))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"V Measure Score :\",v_measure_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2 ... 369  -1 516]\n",
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Calinski Harabasz Score : 20.34504046130372\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Davies Bouldin Score : 4.883935745709393\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Silhouette Score : 0.1700204070337957\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Silhouette Samples : [ 0.4850618   0.81076289  0.63390284 ...  0.87110883 -0.89222697\n",
      "  0.963909  ]\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(\n",
    "    eps=0.5,\n",
    "    min_samples=5,\n",
    "    metric='euclidean',\n",
    "    metric_params=None,\n",
    "    algorithm='auto',\n",
    "    leaf_size=30,\n",
    "    p=None,\n",
    "    n_jobs=None,\n",
    ").fit(X_train)\n",
    "\n",
    "print(dbscan.labels_)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Adjusted Mutual Info Score :\",adjusted_mutual_info_score(X_test)) \n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Adjusted Rand Score :\",adjusted_rand_score(X_test))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Calinski Harabasz Score :\",calinski_harabasz_score(X_train,dbscan.labels_))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Davies Bouldin Score :\",davies_bouldin_score(X_train,dbscan.labels_))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Completeness Score :\",completeness_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Contingency Matrix :\",contingency_matrix(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Fowlkes Mallows Score :\",fowlkes_mallows_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Homogeneity Completeness V Measure :\",homogeneity_completeness_v_measure(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Homogeneity Score :\",homogeneity_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Mutual Info Score :\",mutual_info_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Normalized Mutual Info Score :\",normalized_mutual_info_score(X_test))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Silhouette Score :\",silhouette_score(X_train,dbscan.labels_))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Silhouette Samples :\",silhouette_samples(X_train,dbscan.labels_))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"V Measure Score :\",v_measure_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 1 1 0]\n",
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Calinski Harabasz Score : 23781.015897196357\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Davies Bouldin Score : 0.5953727648396083\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Silhouette Score : 0.5918015713057287\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Silhouette Samples : [0.43616478 0.38290124 0.72894623 ... 0.41220805 0.56112077 0.63895725]\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "aggclus = AgglomerativeClustering(\n",
    "    n_clusters=2,\n",
    "    affinity='euclidean',\n",
    "    memory=None,\n",
    "    connectivity=None,\n",
    "    compute_full_tree='auto',\n",
    "    linkage='ward',\n",
    "    pooling_func='deprecated',\n",
    "    distance_threshold=None,\n",
    ").fit(X_train)\n",
    "\n",
    "print(aggclus.labels_)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Adjusted Mutual Info Score :\",adjusted_mutual_info_score(X_test)) \n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Adjusted Rand Score :\",adjusted_rand_score(X_test))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Calinski Harabasz Score :\",calinski_harabasz_score(X_train,aggclus.labels_))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Davies Bouldin Score :\",davies_bouldin_score(X_train,aggclus.labels_))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Completeness Score :\",completeness_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Contingency Matrix :\",contingency_matrix(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Fowlkes Mallows Score :\",fowlkes_mallows_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Homogeneity Completeness V Measure :\",homogeneity_completeness_v_measure(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Homogeneity Score :\",homogeneity_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Mutual Info Score :\",mutual_info_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Normalized Mutual Info Score :\",normalized_mutual_info_score(X_test))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Silhouette Score :\",silhouette_score(X_train,aggclus.labels_))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Silhouette Samples :\",silhouette_samples(X_train,aggclus.labels_))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"V Measure Score :\",v_measure_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ... 14390   129 14391]\n",
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Calinski Harabasz Score : 34.5155699422817\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Davies Bouldin Score : 0.06305083974612899\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Silhouette Score : 0.0023350075571257484\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Silhouette Samples : [0.         0.         0.         ... 0.         0.08282842 0.        ]\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "affclus = AffinityPropagation(\n",
    "    damping=0.5,\n",
    "    max_iter=5,\n",
    "    convergence_iter=15,\n",
    "    copy=True,\n",
    "    preference=None,\n",
    "    affinity='euclidean',\n",
    "    verbose=False,\n",
    ").fit(X_train)\n",
    "\n",
    "print(affclus.labels_)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Adjusted Mutual Info Score :\",adjusted_mutual_info_score(X_test)) \n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Adjusted Rand Score :\",adjusted_rand_score(X_test))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Calinski Harabasz Score :\",calinski_harabasz_score(X_train,affclus.labels_))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Davies Bouldin Score :\",davies_bouldin_score(X_train,affclus.labels_))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Completeness Score :\",completeness_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Contingency Matrix :\",contingency_matrix(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Fowlkes Mallows Score :\",fowlkes_mallows_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Homogeneity Completeness V Measure :\",homogeneity_completeness_v_measure(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Homogeneity Score :\",homogeneity_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Mutual Info Score :\",mutual_info_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Normalized Mutual Info Score :\",normalized_mutual_info_score(X_test))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Silhouette Score :\",silhouette_score(X_train,affclus.labels_))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Silhouette Samples :\",silhouette_samples(X_train,affclus.labels_))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"V Measure Score :\",v_measure_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTICS Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/sklearn/cluster/optics_.py:795: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  ratio = reachability_plot[:-1] / reachability_plot[1:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   3   8 ... 126  -1 943]\n",
      "*************************************| Error / Accuracy Metrics |************************************\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Calinski Harabasz Score : 20.34504046130372\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Davies Bouldin Score : 4.883935745709393\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Silhouette Score : 0.1700204070337957\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Silhouette Samples : [ 0.4850618   0.81076289  0.63390284 ...  0.87110883 -0.89222697\n",
      "  0.963909  ]\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "optics = OPTICS(\n",
    "    min_samples=5,\n",
    "    max_eps=1,\n",
    "    metric='minkowski',\n",
    "    p=2,\n",
    "    metric_params=None,\n",
    "    cluster_method='xi',\n",
    "    eps=None,\n",
    "    xi=0.05,\n",
    "    predecessor_correction=True,\n",
    "    min_cluster_size=None,\n",
    "    algorithm='auto',\n",
    "    leaf_size=30,\n",
    "    n_jobs=None,\n",
    ").fit(X_train)\n",
    "\n",
    "print(optics.labels_)\n",
    "\n",
    "print(\"*************************************| Error / Accuracy Metrics |************************************\")\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Adjusted Mutual Info Score :\",adjusted_mutual_info_score(X_test)) \n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Adjusted Rand Score :\",adjusted_rand_score(X_test))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Calinski Harabasz Score :\",calinski_harabasz_score(X_train,dbscan.labels_))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Davies Bouldin Score :\",davies_bouldin_score(X_train,dbscan.labels_))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Completeness Score :\",completeness_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Contingency Matrix :\",contingency_matrix(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Fowlkes Mallows Score :\",fowlkes_mallows_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Homogeneity Completeness V Measure :\",homogeneity_completeness_v_measure(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Homogeneity Score :\",homogeneity_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Mutual Info Score :\",mutual_info_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"Normalized Mutual Info Score :\",normalized_mutual_info_score(X_test))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Silhouette Score :\",silhouette_score(X_train,dbscan.labels_))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print(\"Silhouette Samples :\",silhouette_samples(X_train,dbscan.labels_))\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "#print(\"V Measure Score :\",v_measure_score(X_test))\n",
    "#print(\"--------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=20,\n",
    "    algorithm='auto',\n",
    "    leaf_size=30,\n",
    "    metric='minkowski',\n",
    "    p=2,\n",
    "    metric_params=None,\n",
    "    contamination='legacy',\n",
    "    novelty=False,\n",
    "    n_jobs=None,\n",
    ").fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(linreg,X,y, cv=5) \n",
    "print(scores)\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=2)\n",
    "for X_ktrain, X_ktest in kf.split(X):\n",
    "    print(\"%s %s\" % (X_ktrain,X_ktest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=None)\n",
    "for X_rtrain, X_rtest in rkf.split(X):\n",
    "    print(\"%s %s\" % (X_rtrain, X_rtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_stored = pickle.dumps(linreg)\n",
    "model_loaded = pickle.loads(model_stored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "estimators = [(\n",
    "    \n",
    "                'reduce_dim', \n",
    "                PCA(\n",
    "                n_components=None,\n",
    "                copy=True,\n",
    "                whiten=False,\n",
    "                svd_solver='auto',\n",
    "                tol=0.0,\n",
    "                iterated_power='auto',\n",
    "                random_state=None,\n",
    "    )\n",
    "\n",
    "),(\n",
    "    \n",
    "                'linreg', \n",
    "                LinearRegression(\n",
    "                fit_intercept=True,\n",
    "                normalize=False,\n",
    "                copy_X=True,\n",
    "                n_jobs=None,\n",
    "                )\n",
    "\n",
    ")]\n",
    "\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe \n",
    "\n",
    "Pipeline(memory=None,\n",
    "         steps=[('reduce_dim', PCA(copy=True)),\n",
    "                ('linreg', LinearRegression(fit_intercept=True,normalize=False,copy_X=True,n_jobs=None,))], \n",
    "                 verbose=False)\n",
    "\n",
    "print(pipe.steps[0])\n",
    "print(pipe[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuro-linguistic programming (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    input='content',\n",
    "    encoding='utf-8',\n",
    "    decode_error='strict',\n",
    "    strip_accents=None,\n",
    "    lowercase=True,\n",
    "    preprocessor=None,\n",
    "    tokenizer=None,\n",
    "    stop_words=None,\n",
    "    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "    ngram_range=(1, 1),\n",
    "    analyzer='word',\n",
    "    max_df=1.0,\n",
    "    min_df=1,\n",
    "    max_features=None,\n",
    "    vocabulary=None,\n",
    "    binary=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x12 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 20 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['This is an example of NLP',\n",
    "          'This is the first document.',\n",
    "          'And the second one.',\n",
    "          'Is this the first document?']\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze = vectorizer.build_analyzer()\n",
    "analyze(\"This is a text document to analyze.\") == (['this', 'is', 'text', 'document', 'analyze'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()\n",
    "X.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\n",
    "                                     token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "analyze('Bi-grams are cool!') == (['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        1, 1, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46714844, 0.        , 0.        , 0.46714844, 0.        ,\n",
       "        0.25208067, 0.46714844, 0.46714844, 0.        , 0.        ,\n",
       "        0.        , 0.25208067],\n",
       "       [0.        , 0.        , 0.51741994, 0.        , 0.51741994,\n",
       "        0.3935112 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.3935112 , 0.3935112 ],\n",
       "       [0.        , 0.55121857, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.55121857, 0.55121857,\n",
       "        0.29744623, 0.        ],\n",
       "       [0.        , 0.        , 0.51741994, 0.        , 0.51741994,\n",
       "        0.3935112 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.3935112 , 0.3935112 ]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "\n",
    "TfidfTransformer(norm='l2', smooth_idf=False, sublinear_tf=False,\n",
    "                 use_idf=True)\n",
    "\n",
    "tfidf = transformer.fit_transform(X.toarray())\n",
    "tfidf.toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 1, 0, 1, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))\n",
    "counts = ngram_vectorizer.fit_transform(['words', 'wprds'])\n",
    "ngram_vectorizer.get_feature_names() == ([' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'])\n",
    "\n",
    "counts.toarray().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x4 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))\n",
    "ngram_vectorizer.fit_transform(['jumpy fox'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vander Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The food was good.-------------------------------- {'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}\n",
      "The service was not very good!-------------------- {'neg': 0.368, 'neu': 0.632, 'pos': 0.0, 'compound': -0.4432}\n",
      "Not bad at all------------------------------------ {'neg': 0.0, 'neu': 0.513, 'pos': 0.487, 'compound': 0.431}\n",
      "The service was horrible-------------------------- {'neg': 0.538, 'neu': 0.462, 'pos': 0.0, 'compound': -0.5423}\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sentences = [\"The food was good.\",\n",
    "             \"The service was not very good!\", \n",
    "             \"Not bad at all\",\n",
    "             \"The service was horrible\"]\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    print(\"{:-<50} {}\".format(sentence, str(vs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
      "Verbs: ['start', 'work', 'drive', 'take', 'can', 'tell', 'would', 'shake', 'turn', 'be', 'talk', 'say']\n",
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun PERSON\n",
      "Recode ORG\n",
      "earlier this week DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(u\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/myrondza/Documents/Data Science/vdatascience/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: Possible nested set at position 2\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '-', 'world.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "prefix_re = re.compile(r'''^[[(\"']''')\n",
    "suffix_re = re.compile(r'''[])\"']$''')\n",
    "infix_re = re.compile(r'''[-~]''')\n",
    "simple_url_re = re.compile(r'''^https?://''')\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=simple_url_re.match)\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "doc = nlp(u\"hello-world.\")\n",
    "\n",
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['I', 'live', 'in', 'New', 'York']\n",
      "After: ['I', 'live', 'in', 'New York']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(\"I live in New York\")\n",
    "\n",
    "print(\"Before:\", [token.text for token in doc])\n",
    "\n",
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(doc[3:5], attrs={\"LEMMA\": \"new york\"})\n",
    "    \n",
    "print(\"After:\", [token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n",
      "This is another sentence.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(u\"This is a sentence. This is another sentence.\")\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAPPY 😀\n",
      "HASHTAG #MondayMotivation\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = English()  # We only want the tokenizer, so no need to load a model\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pos_emoji = [u\"😀\", u\"😃\", u\"😂\", u\"🤣\", u\"😊\", u\"😍\"]  # Positive emoji\n",
    "neg_emoji = [u\"😞\", u\"😠\", u\"😩\", u\"😢\", u\"😭\", u\"😒\"]  # Negative emoji\n",
    "\n",
    "# Add patterns to match one or more emoji tokens\n",
    "pos_patterns = [[{\"ORTH\": emoji}] for emoji in pos_emoji]\n",
    "neg_patterns = [[{\"ORTH\": emoji}] for emoji in neg_emoji]\n",
    "\n",
    "# Function to label the sentiment\n",
    "def label_sentiment(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    if doc.vocab.strings[match_id] == \"HAPPY\":  # Don't forget to get string!\n",
    "        doc.sentiment += 0.1  # Add 0.1 for positive sentiment\n",
    "    elif doc.vocab.strings[match_id] == \"SAD\":\n",
    "        doc.sentiment -= 0.1  # Subtract 0.1 for negative sentiment\n",
    "\n",
    "matcher.add(\"HAPPY\", label_sentiment, *pos_patterns)  # Add positive pattern\n",
    "matcher.add(\"SAD\", label_sentiment, *neg_patterns)  # Add negative pattern\n",
    "\n",
    "# Add pattern for valid hashtag, i.e. '#' plus any ASCII token\n",
    "matcher.add(\"HASHTAG\", None, [{\"ORTH\": \"#\"}, {\"IS_ASCII\": True}])\n",
    "\n",
    "doc = nlp(u\"Hello world 😀 #MondayMotivation\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = doc.vocab.strings[match_id]  # Look up string ID\n",
    "    span = doc[start:end]\n",
    "    print(string_id, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alex Smith', 'PERSON'), ('first', 'ORDINAL'), ('Acme Corp Inc.', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(\"Dr Alex Smith chaired first board meeting of Acme Corp Inc.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'person': Alex Smith, 'orgs': [Acme Corp Inc.], 'past': True}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"2bc9903d04d940e1bd58cb812c4ff398-0\" class=\"displacy\" width=\"750\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Alex Smith</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">worked</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VBD</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">IN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Acme Corp Inc.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NNP</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2bc9903d04d940e1bd58cb812c4ff398-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2bc9903d04d940e1bd58cb812c4ff398-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2bc9903d04d940e1bd58cb812c4ff398-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2bc9903d04d940e1bd58cb812c4ff398-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M400.0,91.5 L408.0,79.5 392.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2bc9903d04d940e1bd58cb812c4ff398-0-2\" stroke-width=\"2px\" d=\"M420,89.5 C420,2.0 575.0,2.0 575.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2bc9903d04d940e1bd58cb812c4ff398-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,91.5 L583.0,79.5 567.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import merge_entities\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "def extract_person_orgs(doc):\n",
    "    person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    for ent in person_entities:\n",
    "        head = ent.root.head\n",
    "        if head.lemma_ == \"work\":\n",
    "            preps = [token for token in head.children if token.dep_ == \"prep\"]\n",
    "            for prep in preps:\n",
    "                orgs = [token for token in prep.children if token.ent_type_ == \"ORG\"]\n",
    "                print({'person': ent, 'orgs': orgs, 'past': head.tag_ == \"VBD\"})\n",
    "    return doc\n",
    "\n",
    "# To make the entities easier to work with, we'll merge them into single tokens\n",
    "nlp.add_pipe(merge_entities)\n",
    "nlp.add_pipe(extract_person_orgs)\n",
    "\n",
    "doc = nlp(\"Alex Smith worked at Acme Corp Inc.\")\n",
    "# If you're not in a Jupyter / IPython environment, use displacy.serve\n",
    "displacy.render(doc, options={'fine_grained': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">But \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is starting from behind. The company made a late push</br>into hardware, and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Apple’s Siri\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", available on \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    iPhones\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Amazon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "’s Alexa</br>software, which runs on its \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Echo and Dot\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " devices, have clear leads in\n",
       "consumer adoption.</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "text = \"\"\"But Google is starting from behind. The company made a late push\n",
    "into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa\n",
    "software, which runs on its Echo and Dot devices, have clear leads in\n",
    "consumer adoption.\"\"\"\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(text)\n",
    "displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program  :  program\n",
      "programs  :  program\n",
      "programer  :  program\n",
      "programing  :  program\n",
      "programers  :  program\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "   \n",
    "ps = PorterStemmer() \n",
    "  \n",
    "# choose some words to be stemmed \n",
    "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n",
    "  \n",
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programers  :  program\n",
      "program  :  program\n",
      "with  :  with\n",
      "programing  :  program\n",
      "languages  :  languag\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "   \n",
    "ps = PorterStemmer() \n",
    "   \n",
    "sentence = \"Programers program with programing languages\"\n",
    "words = word_tokenize(sentence) \n",
    "   \n",
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n",
      "0.7634336378291241\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "from nltk.corpus import names \n",
    "import nltk \n",
    "#nltk.download('names')\n",
    "  \n",
    "def gender_features(word): \n",
    "    return {'last_letter':word[-1]} \n",
    "  \n",
    "# preparing a list of examples and corresponding class labels. \n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')]+\n",
    "             [(name, 'female') for name in names.words('female.txt')]) \n",
    "  \n",
    "random.shuffle(labeled_names) \n",
    "  \n",
    "# we use the feature extractor to process the names data. \n",
    "featuresets = [(gender_features(n), gender)  \n",
    "               for (n, gender)in labeled_names] \n",
    "  \n",
    "# Divide the resulting list of feature \n",
    "# sets into a training set and a test set. \n",
    "train_set, test_set = featuresets[500:], featuresets[:500] \n",
    "  \n",
    "# The training set is used to  \n",
    "# train a new \"naive Bayes\" classifier. \n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set) \n",
    "  \n",
    "print(classifier.classify(gender_features('Myron'))) \n",
    "  \n",
    "# output should be 'male' \n",
    "print(nltk.classify.accuracy(classifier, train_set)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     35.6 : 1.0\n",
      "             last_letter = 'k'              male : female =     31.7 : 1.0\n",
      "             last_letter = 'f'              male : female =     17.3 : 1.0\n",
      "             last_letter = 'p'              male : female =     11.2 : 1.0\n",
      "             last_letter = 'd'              male : female =     10.7 : 1.0\n",
      "             last_letter = 'v'              male : female =      9.1 : 1.0\n",
      "             last_letter = 'o'              male : female =      8.9 : 1.0\n",
      "             last_letter = 'm'              male : female =      8.7 : 1.0\n",
      "             last_letter = 'r'              male : female =      7.0 : 1.0\n",
      "             last_letter = 'g'              male : female =      4.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "  \n",
    "# a denotes adjective in \"pos\" \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('brown')\n",
    " \n",
    "text = \"The service was good\"\n",
    "\n",
    "blob = TextBlob(text)\n",
    "blob.tags          \n",
    "\n",
    "blob.noun_phrases   \n",
    "for sentence in blob.sentences:\n",
    "    print(sentence.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fallibility', 1.0)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('falibility')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buenos días\n",
      "Bonjour\n",
      "早上好\n",
      "صباح الخير\n"
     ]
    }
   ],
   "source": [
    "en_blob = TextBlob(u'Good Morning')\n",
    "print(en_blob.translate(to='es'))\n",
    "print(en_blob.translate(to='fr'))\n",
    "print(en_blob.translate(to='zh-CN'))\n",
    "print(en_blob.translate(to='ar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ar'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = TextBlob(u\"بسيط هو أفضل من مجمع\")\n",
    "b.detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Now', 'is', 'better']),\n",
       " WordList(['is', 'better', 'than']),\n",
       " WordList(['better', 'than', 'never'])]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"Now is better than never.\")\n",
    "blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('chop.v.05'),\n",
       " Synset('hack.v.02'),\n",
       " Synset('hack.v.03'),\n",
       " Synset('hack.v.04'),\n",
       " Synset('hack.v.05'),\n",
       " Synset('hack.v.06'),\n",
       " Synset('hack.v.07'),\n",
       " Synset('hack.v.08')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "from textblob.wordnet import VERB\n",
    "word = Word(\"octopus\")\n",
    "word.synsets\n",
    "Word(\"hack\").get_synsets(pos=VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['water falling in drops from vapor condensed in the atmosphere',\n",
       " 'drops of fresh water that fall as precipitation from clouds',\n",
       " 'anything happening rapidly or in quick successive',\n",
       " 'precipitate as rain']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " Word(\"rain\").definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow + Keras (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14480, 4)\n",
      "(14480,)\n",
      "(7133, 4)\n",
      "(7133,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print (X_test.shape)\n",
    "print (y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.Dense(\n",
    "    units,\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros',\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.Conv1D(\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    strides=1,\n",
    "    padding='valid',\n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros',\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None\n",
    ")\n",
    "\n",
    "tf.keras.layers.Conv2D(\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    strides=(1, 1),\n",
    "    padding='valid',\n",
    "    data_format=None,\n",
    "    dilation_rate=(1, 1),\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros',\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.MaxPooling1D(\n",
    "    pool_size=2,\n",
    "    strides=None,\n",
    "    padding='valid',\n",
    "    data_format='channels_last'\n",
    ")\n",
    "\n",
    "tf.keras.layers.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    strides=None,\n",
    "    padding='valid',\n",
    "    data_format=None\n",
    ")\n",
    "\n",
    "tf.keras.layers.MaxPooling3D(\n",
    "    pool_size=(2, 2, 2),\n",
    "    strides=None,\n",
    "    padding='valid',\n",
    "    data_format=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally Connected Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.LocallyConnected1D(\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    strides=1,\n",
    "    padding='valid',\n",
    "    data_format=None,\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros',\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    implementation=1\n",
    ")\n",
    "\n",
    "tf.keras.layers.LocallyConnected2D(\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    strides=(1, 1),\n",
    "    padding='valid',\n",
    "    data_format=None,\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros',\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    implementation=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64, (3, 3),input_shape=(3, 32, 32), padding='same',))\n",
    "# now: model.output_shape == (None, 64, 32, 32)\n",
    "model.add(Flatten())\n",
    "# now: model.output_shape == (None, 65536)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.RNN(\n",
    "    cell,\n",
    "    return_sequences=False,\n",
    "    return_state=False,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    "    unroll=False,\n",
    "    time_major=False\n",
    ")\n",
    "\n",
    "tf.keras.layers.SimpleRNN(\n",
    "    units,\n",
    "    activation='tanh',\n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    recurrent_initializer='orthogonal',\n",
    "    bias_initializer='zeros',\n",
    "    kernel_regularizer=None,\n",
    "    recurrent_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    recurrent_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    return_sequences=False,\n",
    "    return_state=False,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    "    unroll=False\n",
    ")\n",
    "\n",
    "tf.keras.layers.GRU(\n",
    "    units,\n",
    "    activation='tanh',\n",
    "    recurrent_activation='sigmoid',\n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    recurrent_initializer='orthogonal',\n",
    "    bias_initializer='zeros',\n",
    "    kernel_regularizer=None,\n",
    "    recurrent_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    recurrent_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    implementation=2,\n",
    "    return_sequences=False,\n",
    "    return_state=False,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    "    unroll=False,\n",
    "    time_major=False,\n",
    "    reset_after=True\n",
    ")\n",
    "\n",
    "tf.keras.layers.LSTM(\n",
    "    units,\n",
    "    activation='tanh',\n",
    "    recurrent_activation='sigmoid',\n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    recurrent_initializer='orthogonal',\n",
    "    bias_initializer='zeros',\n",
    "    unit_forget_bias=True,\n",
    "    kernel_regularizer=None,\n",
    "    recurrent_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    recurrent_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    implementation=2,\n",
    "    return_sequences=False,\n",
    "    return_state=False,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    "    time_major=False,\n",
    "    unroll=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Activation Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.advanced_activations.ThresholdedReLU at 0x14db90c50>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)\n",
    "tf.keras.layers.ELU(alpha=1.0) # Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\n",
    "tf.keras.layers.Softmax(axis=-1)\n",
    "tf.keras.layers.LeakyReLU(alpha=0.3) #Rectifier Nonlinearities Improve Neural Network Acoustic Models\n",
    "tf.keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None) #Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n",
    "tf.keras.layers.ThresholdedReLU(theta=1.0) #Zero-Bias Autoencoders and the Benefits of Co-Adapting Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer - A Theoretically Grounded Application of Dropout in Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerating Deep Network Training (Reducing Internal Covariate Shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x14db90f28>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.BatchNormalization(\n",
    "    axis=-1,\n",
    "    momentum=0.99,\n",
    "    epsilon=0.001,\n",
    "    center=True,\n",
    "    scale=True,\n",
    "    beta_initializer='zeros',\n",
    "    gamma_initializer='ones',\n",
    "    moving_mean_initializer='zeros',\n",
    "    moving_variance_initializer='ones',\n",
    "    beta_regularizer=None,\n",
    "    gamma_regularizer=None,\n",
    "    beta_constraint=None,\n",
    "    gamma_constraint=None,\n",
    "    renorm=False,\n",
    "    renorm_clipping=None,\n",
    "    renorm_momentum=0.99,\n",
    "    fused=None,\n",
    "    trainable=True,\n",
    "    virtual_batch_size=None,\n",
    "    adjustment=None,\n",
    "    name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prevent Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.Dropout(rate, noise_shape=None, seed=None, **kwargs)\n",
    "tf.keras.layers.GaussianNoise(stddev)\n",
    "tf.keras.layers.GaussianDropout(rate)\n",
    "tf.keras.layers.AlphaDropout(rate, noise_shape=None, seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.activations.elu(x, alpha=1.0)\n",
    "tf.keras.activations.exponential(x)\n",
    "tf.keras.activations.hard_sigmoid(x)\n",
    "tf.keras.activations.linear(x)\n",
    "tf.keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0)\n",
    "tf.keras.activations.selu(x)\n",
    "tf.keras.activations.sigmoid(x)\n",
    "tf.keras.activations.tanh(x)\n",
    "tf.keras.activations.softmax(x, axis=-1)\n",
    "tf.keras.activations.softplus(x)\n",
    "tf.keras.activations.softsign(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A linear layer with L1 regularization of factor 0.01 applied to the kernel matrix:\n",
    "layers.Dense(64, kernel_regularizer=tf.keras.regularizers.l1(0.01))\n",
    "\n",
    "# A linear layer with L2 regularization of factor 0.01 applied to the bias vector:\n",
    "layers.Dense(64, bias_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "\n",
    "# A linear layer with L1 & L2 regularization of factor 0.01 applied to the kernel matrix:\n",
    "layers.Dense(64, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.metrics.CategoricalCrossentropy at 0x14d8d7240>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.metrics.CategoricalAccuracy()\n",
    "tf.keras.metrics.Accuracy()\n",
    "tf.keras.metrics.AUC()\n",
    "tf.keras.metrics.BinaryAccuracy()\n",
    "tf.keras.metrics.BinaryCrossentropy()\n",
    "tf.keras.metrics.CategoricalAccuracy()\n",
    "tf.keras.metrics.CategoricalCrossentropy()\n",
    "tf.keras.metrics.CategoricalHinge()\n",
    "tf.keras.metrics.CosineSimilarity()\n",
    "tf.keras.metrics.Hinge()\n",
    "\n",
    "tf.keras.metrics.KLD(y_true, y_pred)\n",
    "tf.keras.metrics.MAE(y_true, y_pred)\n",
    "tf.keras.metrics.MAPE(y_true, y_pred)\n",
    "tf.keras.metrics.Mean()\n",
    "tf.keras.metrics.MSE(y_true, y_pred)\n",
    "tf.keras.metrics.MSLE(y_true, y_pred)\n",
    "\n",
    "tf.keras.metrics.Poisson()\n",
    "tf.keras.metrics.RootMeanSquaredError()\n",
    "tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=False,\n",
    "    label_smoothing=0,\n",
    "    reduction='auto',\n",
    "    name='categorical_crossentropy',\n",
    ")\n",
    "\n",
    "tf.keras.losses.BinaryCrossentropy(\n",
    "    from_logits=False,\n",
    "    label_smoothing=0,\n",
    "    reduction='auto',\n",
    "    name='binary_crossentropy',\n",
    ")\n",
    "\n",
    "tf.keras.losses.CategoricalHinge(\n",
    "    reduction='auto',\n",
    "    name='categorical_hinge',\n",
    ")\n",
    "\n",
    "tf.keras.losses.CosineSimilarity(\n",
    "    axis=-1,\n",
    "    reduction='auto',\n",
    "    name='cosine_similarity',\n",
    ")\n",
    "\n",
    "tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False,\n",
    "    reduction='auto',\n",
    "    name=None,\n",
    ")\n",
    "\n",
    "tf.keras.losses.KLD(y_true, y_pred)\n",
    "tf.keras.losses.LogCosh(reduction='auto', name='logcosh')\n",
    "tf.keras.losses.Poisson(reduction='auto', name='poisson')\n",
    "tf.keras.losses.SquaredHinge(reduction='auto', name='squared_hinge')\n",
    "\n",
    "tf.keras.losses.MAE(y_true, y_pred)\n",
    "tf.keras.losses.MAPE(y_true, y_pred)\n",
    "tf.keras.losses.MSE(y_true, y_pred)\n",
    "tf.keras.losses.MSLE(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl at 0x14cdec160>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.optimizers.Adadelta(\n",
    "    learning_rate=0.001,\n",
    "    rho=0.95,\n",
    "    epsilon=1e-07,\n",
    "    name='Adadelta'\n",
    ")\n",
    "\n",
    "tf.keras.optimizers.Adagrad(\n",
    "    learning_rate=0.001,\n",
    "    initial_accumulator_value=0.1,\n",
    "    epsilon=1e-07,\n",
    "    name='Adagrad'\n",
    ")\n",
    "\n",
    "tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False,\n",
    "    name='Adam'\n",
    ")\n",
    "\n",
    "tf.keras.optimizers.Adamax(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    name='Adamax'\n",
    ")\n",
    "\n",
    "tf.keras.optimizers.Nadam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    name='Nadam'\n",
    ")\n",
    "\n",
    "tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.01,\n",
    "    momentum=0.0,\n",
    "    nesterov=False,\n",
    "    name='SGD'\n",
    ")\n",
    "\n",
    "tf.keras.optimizers.Ftrl(\n",
    "    learning_rate=0.001,\n",
    "    learning_rate_power=-0.5,\n",
    "    initial_accumulator_value=0.1,\n",
    "    l1_regularization_strength=0.0,\n",
    "    l2_regularization_strength=0.0,\n",
    "    name='Ftrl',\n",
    "    l2_shrinkage_regularization_strength=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network (DNN) for Regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14480 samples, validate on 7133 samples\n",
      "Epoch 1/100\n",
      "14480/14480 [==============================] - 1s 44us/sample - loss: 413171404813.5779 - accuracy: 0.0000e+00 - val_loss: 440752549828.8541 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "14480/14480 [==============================] - 0s 6us/sample - loss: 409316945272.7867 - accuracy: 0.0000e+00 - val_loss: 436446686449.3212 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 404698649458.5635 - accuracy: 0.0000e+00 - val_loss: 430971976680.6000 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 398638294322.6342 - accuracy: 0.0000e+00 - val_loss: 423456072880.1458 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 390571156116.7911 - accuracy: 0.0000e+00 - val_loss: 414098033959.2990 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 380118885048.9989 - accuracy: 0.0000e+00 - val_loss: 401366873238.3054 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 366842360373.7458 - accuracy: 0.0000e+00 - val_loss: 386363749333.3632 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 350931781792.6718 - accuracy: 0.0000e+00 - val_loss: 367793431057.4423 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 331975484559.6995 - accuracy: 0.0000e+00 - val_loss: 346736291360.5159 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 310330520063.4343 - accuracy: 0.0000e+00 - val_loss: 322504893825.1664 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "14480/14480 [==============================] - 0s 6us/sample - loss: 285872017819.8630 - accuracy: 0.0000e+00 - val_loss: 295930267529.4210 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "14480/14480 [==============================] - 0s 7us/sample - loss: 259329094976.2122 - accuracy: 0.0000e+00 - val_loss: 266644717375.0579 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "14480/14480 [==============================] - 0s 6us/sample - loss: 230622054198.5945 - accuracy: 0.0000e+00 - val_loss: 236146935445.9464 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 200860055862.0287 - accuracy: 0.0000e+00 - val_loss: 205013014748.7924 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "14480/14480 [==============================] - 0s 6us/sample - loss: 170927180878.0729 - accuracy: 0.0000e+00 - val_loss: 173552445396.0712 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 142043425100.6586 - accuracy: 0.0000e+00 - val_loss: 145655209348.8989 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 116361339871.1868 - accuracy: 0.0000e+00 - val_loss: 120338951758.4545 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 94272626285.1889 - accuracy: 0.0000e+00 - val_loss: 99742456743.1375 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 78016912466.5989 - accuracy: 0.0000e+00 - val_loss: 85665340028.5367 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 68344187986.5989 - accuracy: 0.0000e+00 - val_loss: 79022787278.6519 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64985806234.7315 - accuracy: 0.0000e+00 - val_loss: 77362043077.8231 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64533557958.5768 - accuracy: 0.0000e+00 - val_loss: 77257949290.8072 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64536008837.5160 - accuracy: 0.0000e+00 - val_loss: 77349886502.4018 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64528405308.2519 - accuracy: 0.0000e+00 - val_loss: 77231070423.4807 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64511885425.1492 - accuracy: 0.0000e+00 - val_loss: 77167484445.6447 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64574600055.0895 - accuracy: 0.0000e+00 - val_loss: 77352721835.9467 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "14480/14480 [==============================] - 0s 6us/sample - loss: 64516575675.5448 - accuracy: 0.0000e+00 - val_loss: 77688796914.3979 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64552747112.0972 - accuracy: 0.0000e+00 - val_loss: 77767840263.1061 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64551166087.7790 - accuracy: 0.0000e+00 - val_loss: 77188953055.6994 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64539658977.7326 - accuracy: 0.0000e+00 - val_loss: 77688983812.1273 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64583474643.3061 - accuracy: 0.0000e+00 - val_loss: 77340692660.8832 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64547501643.2442 - accuracy: 0.0000e+00 - val_loss: 77340543897.9302 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64513836026.3425 - accuracy: 0.0000e+00 - val_loss: 77420827139.0865 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64554036813.5072 - accuracy: 0.0000e+00 - val_loss: 77408055231.1117 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64478540644.9856 - accuracy: 0.0000e+00 - val_loss: 77380637501.7659 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64457851812.3492 - accuracy: 0.0000e+00 - val_loss: 77232291551.7353 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64584698668.4111 - accuracy: 0.0000e+00 - val_loss: 77230827284.2776 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64578162176.5658 - accuracy: 0.0000e+00 - val_loss: 77160226411.1661 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64539776899.5359 - accuracy: 0.0000e+00 - val_loss: 77164253051.3523 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64544058966.5591 - accuracy: 0.0000e+00 - val_loss: 77609232182.1573 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64493472207.9116 - accuracy: 0.0000e+00 - val_loss: 77242477151.2508 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64532042701.0829 - accuracy: 0.0000e+00 - val_loss: 77354205147.2491 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64544649754.5901 - accuracy: 0.0000e+00 - val_loss: 77198601899.4802 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64539067807.2575 - accuracy: 0.0000e+00 - val_loss: 77331518831.3652 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64529727506.1039 - accuracy: 0.0000e+00 - val_loss: 77249871158.8033 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64515800811.9160 - accuracy: 0.0000e+00 - val_loss: 77245411586.9788 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64540556316.2873 - accuracy: 0.0000e+00 - val_loss: 77498688069.6975 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64509429952.3536 - accuracy: 0.0000e+00 - val_loss: 77319753211.7650 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "14480/14480 [==============================] - 0s 6us/sample - loss: 64527736587.5978 - accuracy: 0.0000e+00 - val_loss: 77235674513.5320 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "14480/14480 [==============================] - 0s 6us/sample - loss: 64517779541.9934 - accuracy: 0.0000e+00 - val_loss: 77431654337.6958 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "14480/14480 [==============================] - 0s 6us/sample - loss: 64499461270.4884 - accuracy: 0.0000e+00 - val_loss: 77162662682.4506 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "14480/14480 [==============================] - 0s 6us/sample - loss: 64520712603.8630 - accuracy: 0.0000e+00 - val_loss: 77514614480.5182 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "14480/14480 [==============================] - 0s 6us/sample - loss: 64532223486.3028 - accuracy: 0.0000e+00 - val_loss: 77142040564.5154 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64506515976.4862 - accuracy: 0.0000e+00 - val_loss: 77464366696.8692 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "14480/14480 [==============================] - 0s 6us/sample - loss: 64508365168.8663 - accuracy: 0.0000e+00 - val_loss: 77142674189.2432 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "14480/14480 [==============================] - 0s 7us/sample - loss: 64517530324.1547 - accuracy: 0.0000e+00 - val_loss: 77136806434.6693 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/100\n",
      "14480/14480 [==============================] - 0s 6us/sample - loss: 64536174967.6553 - accuracy: 0.0000e+00 - val_loss: 77155684267.5878 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64470780883.8718 - accuracy: 0.0000e+00 - val_loss: 77463998732.1665 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64540288415.2575 - accuracy: 0.0000e+00 - val_loss: 77134377018.5717 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "14480/14480 [==============================] - 0s 7us/sample - loss: 64530911508.0840 - accuracy: 0.0000e+00 - val_loss: 77713900219.4151 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "14480/14480 [==============================] - 0s 6us/sample - loss: 64547247537.3613 - accuracy: 0.0000e+00 - val_loss: 77142307739.9400 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64522488095.3989 - accuracy: 0.0000e+00 - val_loss: 77514357760.7178 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64540468756.9326 - accuracy: 0.0000e+00 - val_loss: 77180399923.2144 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64505330250.1127 - accuracy: 0.0000e+00 - val_loss: 77260468208.4957 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64492147081.7591 - accuracy: 0.0000e+00 - val_loss: 77192747737.5623 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64535078140.3227 - accuracy: 0.0000e+00 - val_loss: 77218036701.1154 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64476922598.2586 - accuracy: 0.0000e+00 - val_loss: 77278736546.5078 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64505933421.1889 - accuracy: 0.0000e+00 - val_loss: 77212675636.0398 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64518950409.6177 - accuracy: 0.0000e+00 - val_loss: 77180519740.2585 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64563572935.1425 - accuracy: 0.0000e+00 - val_loss: 77296878161.7563 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64480453558.4530 - accuracy: 0.0000e+00 - val_loss: 77381546082.1937 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64522782061.4718 - accuracy: 0.0000e+00 - val_loss: 77246518010.0065 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64498598892.7646 - accuracy: 0.0000e+00 - val_loss: 77250206825.2281 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64485736923.2265 - accuracy: 0.0000e+00 - val_loss: 77127532805.8500 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64454857256.1680 - accuracy: 0.0000e+00 - val_loss: 77646878112.7492 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64532189826.6873 - accuracy: 0.0000e+00 - val_loss: 77470058871.2609 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64496889480.3447 - accuracy: 0.0000e+00 - val_loss: 77294353783.6916 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "14480/14480 [==============================] - 0s 6us/sample - loss: 64531667190.6652 - accuracy: 0.0000e+00 - val_loss: 77616163218.8241 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64511917935.1691 - accuracy: 0.0000e+00 - val_loss: 77159708659.6540 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64491554329.4586 - accuracy: 0.0000e+00 - val_loss: 77370798535.3663 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64476310737.3260 - accuracy: 0.0000e+00 - val_loss: 77630854822.5992 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64368255564.3757 - accuracy: 0.0000e+00 - val_loss: 77148584581.7244 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64483424412.1459 - accuracy: 0.0000e+00 - val_loss: 77527131309.8489 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64524875597.2243 - accuracy: 0.0000e+00 - val_loss: 77118476308.0981 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64472339119.9470 - accuracy: 0.0000e+00 - val_loss: 77113216317.6941 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64469476622.4265 - accuracy: 0.0000e+00 - val_loss: 77290552134.6665 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64527516584.8751 - accuracy: 0.0000e+00 - val_loss: 77234878192.1009 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64515116481.2022 - accuracy: 0.0000e+00 - val_loss: 77116094003.7527 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64513994394.4486 - accuracy: 0.0000e+00 - val_loss: 77319774072.4811 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "14480/14480 [==============================] - 0s 6us/sample - loss: 64505982921.6884 - accuracy: 0.0000e+00 - val_loss: 77116577309.3576 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64480544915.0939 - accuracy: 0.0000e+00 - val_loss: 77553507845.5270 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64522432448.6365 - accuracy: 0.0000e+00 - val_loss: 77496272024.0280 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64515583040.4950 - accuracy: 0.0000e+00 - val_loss: 77369231878.8190 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64480705971.6243 - accuracy: 0.0000e+00 - val_loss: 77156067116.3953 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64531493194.3956 - accuracy: 0.0000e+00 - val_loss: 77250532704.4352 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64504708115.2354 - accuracy: 0.0000e+00 - val_loss: 77332722714.2711 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64495857949.1359 - accuracy: 0.0000e+00 - val_loss: 77499697062.1326 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64528426580.2961 - accuracy: 0.0000e+00 - val_loss: 77307887153.7429 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64485879817.0519 - accuracy: 0.0000e+00 - val_loss: 77246161804.4357 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "14480/14480 [==============================] - 0s 5us/sample - loss: 64491896474.4486 - accuracy: 0.0000e+00 - val_loss: 77328051517.1199 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[535730.3 ],\n",
       "       [750693.25],\n",
       "       [976612.4 ],\n",
       "       ...,\n",
       "       [416418.94],\n",
       "       [377953.25],\n",
       "       [323322.7 ]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=1)\n",
    "\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./Graph',\n",
    "                                 write_graph=True,\n",
    "                                 histogram_freq=1)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "def regression_model() :   \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(64,input_dim=4))\n",
    "    model.add(layers.Dense(32,activation=tf.nn.relu))\n",
    "    model.add(layers.Dense(16,activation=tf.nn.relu))\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),loss = \"mse\",metrics =[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = regression_model()\n",
    "model.fit(X_train,y_train,epochs=100,batch_size=1024,validation_data=(X_test,y_test),callbacks=[tensorboard_callback])\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network (DNN) for Binary Classification :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14480 samples, validate on 7133 samples\n",
      "Epoch 1/100\n",
      "14480/14480 [==============================] - 1s 39us/sample - loss: 8295368.5436 - accuracy: 0.0000e+00 - val_loss: 8402822.8707 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.2729 - accuracy: 0.0000e+00 - val_loss: 8402822.3857 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4691 - accuracy: 0.0000e+00 - val_loss: 8402822.7523 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4298 - accuracy: 0.0000e+00 - val_loss: 8402822.7397 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.2298 - accuracy: 0.0000e+00 - val_loss: 8402822.7931 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.6033 - accuracy: 0.0000e+00 - val_loss: 8402822.5061 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4530 - accuracy: 0.0000e+00 - val_loss: 8402822.8906 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.3238 - accuracy: 0.0000e+00 - val_loss: 8402822.7584 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4254 - accuracy: 0.0000e+00 - val_loss: 8402822.5364 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.3796 - accuracy: 0.0000e+00 - val_loss: 8402822.4399 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.5370 - accuracy: 0.0000e+00 - val_loss: 8402822.7419 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4442 - accuracy: 0.0000e+00 - val_loss: 8402822.9446 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.6094 - accuracy: 0.0000e+00 - val_loss: 8402822.4560 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4685 - accuracy: 0.0000e+00 - val_loss: 8402822.2721 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "14480/14480 [==============================] - 0s 7us/sample - loss: 8295368.1481 - accuracy: 0.0000e+00 - val_loss: 8402822.8129 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 8295368.3481 - accuracy: 0.0000e+00 - val_loss: 8402822.6361 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 8295368.4729 - accuracy: 0.0000e+00 - val_loss: 8402822.4647 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.6961 - accuracy: 0.0000e+00 - val_loss: 8402822.9023 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.2696 - accuracy: 0.0000e+00 - val_loss: 8402822.4947 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.3624 - accuracy: 0.0000e+00 - val_loss: 8402822.5883 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.3442 - accuracy: 0.0000e+00 - val_loss: 8402822.5619 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4680 - accuracy: 0.0000e+00 - val_loss: 8402822.5850 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "14480/14480 [==============================] - 0s 7us/sample - loss: 8295368.3812 - accuracy: 0.0000e+00 - val_loss: 8402822.5723 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "14480/14480 [==============================] - 0s 7us/sample - loss: 8295368.4149 - accuracy: 0.0000e+00 - val_loss: 8402822.6846 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/100\n",
      "14480/14480 [==============================] - 0s 7us/sample - loss: 8295368.3961 - accuracy: 0.0000e+00 - val_loss: 8402822.8864 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "14480/14480 [==============================] - 0s 7us/sample - loss: 8295368.3906 - accuracy: 0.0000e+00 - val_loss: 8402822.7906 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.3956 - accuracy: 0.0000e+00 - val_loss: 8402823.0008 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "14480/14480 [==============================] - 0s 7us/sample - loss: 8295368.1845 - accuracy: 0.0000e+00 - val_loss: 8402822.5109 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.2751 - accuracy: 0.0000e+00 - val_loss: 8402822.5022 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4354 - accuracy: 0.0000e+00 - val_loss: 8402822.7374 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.6829 - accuracy: 0.0000e+00 - val_loss: 8402822.5280 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4674 - accuracy: 0.0000e+00 - val_loss: 8402822.8786 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.5105 - accuracy: 0.0000e+00 - val_loss: 8402822.7837 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "14480/14480 [==============================] - 0s 7us/sample - loss: 8295368.4856 - accuracy: 0.0000e+00 - val_loss: 8402822.5204 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "14480/14480 [==============================] - 0s 7us/sample - loss: 8295368.5409 - accuracy: 0.0000e+00 - val_loss: 8402822.6317 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "14480/14480 [==============================] - 0s 7us/sample - loss: 8295368.2227 - accuracy: 0.0000e+00 - val_loss: 8402822.9075 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4144 - accuracy: 0.0000e+00 - val_loss: 8402822.6752 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4188 - accuracy: 0.0000e+00 - val_loss: 8402822.5311 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "14480/14480 [==============================] - 0s 7us/sample - loss: 8295368.3591 - accuracy: 0.0000e+00 - val_loss: 8402822.7861 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4878 - accuracy: 0.0000e+00 - val_loss: 8402822.7582 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4840 - accuracy: 0.0000e+00 - val_loss: 8402822.2678 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.2033 - accuracy: 0.0000e+00 - val_loss: 8402822.3530 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.5044 - accuracy: 0.0000e+00 - val_loss: 8402822.6345 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4530 - accuracy: 0.0000e+00 - val_loss: 8402822.3642 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4337 - accuracy: 0.0000e+00 - val_loss: 8402822.9141 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.1923 - accuracy: 0.0000e+00 - val_loss: 8402823.0918 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4083 - accuracy: 0.0000e+00 - val_loss: 8402822.9626 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.1785 - accuracy: 0.0000e+00 - val_loss: 8402822.6171 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4177 - accuracy: 0.0000e+00 - val_loss: 8402822.4830 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.5586 - accuracy: 0.0000e+00 - val_loss: 8402822.9157 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4431 - accuracy: 0.0000e+00 - val_loss: 8402822.5925 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.3072 - accuracy: 0.0000e+00 - val_loss: 8402822.5470 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.3116 - accuracy: 0.0000e+00 - val_loss: 8402822.5311 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.3536 - accuracy: 0.0000e+00 - val_loss: 8402822.7465 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4884 - accuracy: 0.0000e+00 - val_loss: 8402822.8428 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4707 - accuracy: 0.0000e+00 - val_loss: 8402822.7341 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.6420 - accuracy: 0.0000e+00 - val_loss: 8402822.7165 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.2199 - accuracy: 0.0000e+00 - val_loss: 8402822.7922 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.3646 - accuracy: 0.0000e+00 - val_loss: 8402822.8089 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4862 - accuracy: 0.0000e+00 - val_loss: 8402822.7189 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.5287 - accuracy: 0.0000e+00 - val_loss: 8402822.6590 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.3536 - accuracy: 0.0000e+00 - val_loss: 8402822.7969 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.3961 - accuracy: 0.0000e+00 - val_loss: 8402822.5271 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.3138 - accuracy: 0.0000e+00 - val_loss: 8402822.1523 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4061 - accuracy: 0.0000e+00 - val_loss: 8402822.7015 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "14480/14480 [==============================] - 0s 10us/sample - loss: 8295368.5044 - accuracy: 0.0000e+00 - val_loss: 8402822.6449 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "14480/14480 [==============================] - 0s 7us/sample - loss: 8295368.2022 - accuracy: 0.0000e+00 - val_loss: 8402823.1485 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.3315 - accuracy: 0.0000e+00 - val_loss: 8402822.8184 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.5204 - accuracy: 0.0000e+00 - val_loss: 8402822.6126 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.5238 - accuracy: 0.0000e+00 - val_loss: 8402822.8909 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.3431 - accuracy: 0.0000e+00 - val_loss: 8402822.9244 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4177 - accuracy: 0.0000e+00 - val_loss: 8402822.6037 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4000 - accuracy: 0.0000e+00 - val_loss: 8402822.4586 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4265 - accuracy: 0.0000e+00 - val_loss: 8402822.9352 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.5033 - accuracy: 0.0000e+00 - val_loss: 8402822.4707 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4669 - accuracy: 0.0000e+00 - val_loss: 8402822.7307 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.5215 - accuracy: 0.0000e+00 - val_loss: 8402822.4659 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "14480/14480 [==============================] - 0s 7us/sample - loss: 8295368.4160 - accuracy: 0.0000e+00 - val_loss: 8402822.4670 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4320 - accuracy: 0.0000e+00 - val_loss: 8402822.8773 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.2989 - accuracy: 0.0000e+00 - val_loss: 8402822.7346 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.2608 - accuracy: 0.0000e+00 - val_loss: 8402822.5990 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.5381 - accuracy: 0.0000e+00 - val_loss: 8402822.7458 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 8295368.3983 - accuracy: 0.0000e+00 - val_loss: 8402822.9714 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.5204 - accuracy: 0.0000e+00 - val_loss: 8402823.1312 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4155 - accuracy: 0.0000e+00 - val_loss: 8402822.9336 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "14480/14480 [==============================] - 0s 7us/sample - loss: 8295368.2580 - accuracy: 0.0000e+00 - val_loss: 8402822.7873 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.5050 - accuracy: 0.0000e+00 - val_loss: 8402823.0310 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4552 - accuracy: 0.0000e+00 - val_loss: 8402822.4691 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.3823 - accuracy: 0.0000e+00 - val_loss: 8402822.7336 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4166 - accuracy: 0.0000e+00 - val_loss: 8402822.7264 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.3818 - accuracy: 0.0000e+00 - val_loss: 8402822.6480 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4530 - accuracy: 0.0000e+00 - val_loss: 8402822.6267 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/100\n",
      "14480/14480 [==============================] - 0s 7us/sample - loss: 8295368.3602 - accuracy: 0.0000e+00 - val_loss: 8402822.5260 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4265 - accuracy: 0.0000e+00 - val_loss: 8402822.3326 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.1669 - accuracy: 0.0000e+00 - val_loss: 8402822.9249 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.4851 - accuracy: 0.0000e+00 - val_loss: 8402822.7352 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14480/14480 [==============================] - 0s 7us/sample - loss: 8295368.5182 - accuracy: 0.0000e+00 - val_loss: 8402822.5558 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 8295368.4110 - accuracy: 0.0000e+00 - val_loss: 8402822.6736 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.1492 - accuracy: 0.0000e+00 - val_loss: 8402822.4661 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 8295368.3878 - accuracy: 0.0000e+00 - val_loss: 8402822.6316 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=1)\n",
    "\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./Graph',\n",
    "                                 write_graph=True,\n",
    "                                 histogram_freq=1)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "def binaryclass_model() :   \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(128,input_dim=4))\n",
    "    model.add(layers.Dense(64,activation=tf.nn.relu))\n",
    "    model.add(layers.Dense(32,activation=tf.nn.relu))\n",
    "    model.add(layers.Dense(1,activation=tf.nn.sigmoid))  \n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01),loss = 'binary_crossentropy',metrics =[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = binaryclass_model()\n",
    "model.fit(X_train,y_train,epochs=100,batch_size=1024,validation_data=(X_test,y_test),callbacks=[tensorboard_callback])\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network (DNN) for Multi-Class Classification :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14480 samples, validate on 7133 samples\n",
      "Epoch 1/100\n",
      "14480/14480 [==============================] - 1s 39us/sample - loss: 9.8743 - accuracy: 0.3874 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 2/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 3/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 4/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 5/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 6/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 7/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 8/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 9/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 10/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 11/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 12/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 13/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 14/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 15/100\n",
      "14480/14480 [==============================] - 0s 12us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 16/100\n",
      "14480/14480 [==============================] - 0s 14us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 17/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 18/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 19/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 20/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 21/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 22/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 23/100\n",
      "14480/14480 [==============================] - 0s 10us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 24/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 25/100\n",
      "14480/14480 [==============================] - 0s 10us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 26/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 27/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 28/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 29/100\n",
      "14480/14480 [==============================] - 0s 10us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 30/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 31/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 32/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 33/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 34/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 35/100\n",
      "14480/14480 [==============================] - 0s 10us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 36/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 37/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 38/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 39/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 40/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 41/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 42/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 43/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 44/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 45/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 46/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 47/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 48/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 49/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 50/100\n",
      "14480/14480 [==============================] - ETA: 0s - loss: 9.4092 - accuracy: 0.41 - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 51/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 52/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 53/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 54/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 56/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 57/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 58/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 59/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 60/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 61/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 62/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 63/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 64/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 65/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 66/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 67/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 68/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 69/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 70/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 71/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 72/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 73/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 74/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 75/100\n",
      "14480/14480 [==============================] - 0s 10us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 76/100\n",
      "14480/14480 [==============================] - 0s 10us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 77/100\n",
      "14480/14480 [==============================] - 0s 10us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 78/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 79/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 80/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 81/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 82/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 83/100\n",
      "14480/14480 [==============================] - 0s 10us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 84/100\n",
      "14480/14480 [==============================] - 0s 10us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 85/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 86/100\n",
      "14480/14480 [==============================] - 0s 8us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 87/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 88/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 89/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 90/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 91/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 92/100\n",
      "14480/14480 [==============================] - 0s 11us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 93/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 94/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 95/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 96/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 97/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 98/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 99/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n",
      "Epoch 100/100\n",
      "14480/14480 [==============================] - 0s 9us/sample - loss: 9.4182 - accuracy: 0.4157 - val_loss: 9.4250 - val_accuracy: 0.4153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=1)\n",
    "\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./Graph',\n",
    "                                 write_graph=True,\n",
    "                                 histogram_freq=1)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "def multiclass_model() :   \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(128,input_dim=4))\n",
    "    model.add(layers.Dense(64,activation=tf.nn.relu))\n",
    "    model.add(layers.Dense(32,activation=tf.nn.relu))\n",
    "    model.add(layers.Dense(14,activation=tf.nn.softmax))  \n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01),loss = 'sparse_categorical_crossentropy',metrics =[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = multiclass_model()\n",
    "model.fit(X_train,y_train,epochs=100,batch_size=1024,validation_data=(X_test,y_test),callbacks=[tensorboard_callback])\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network (CNN) for Multi-Class Classification :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 5s 46ms/sample - loss: 2.3914\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 4s 40ms/sample - loss: 2.3216\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 4s 40ms/sample - loss: 2.2927\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 4s 40ms/sample - loss: 2.3059\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 4s 40ms/sample - loss: 2.2892\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 4s 41ms/sample - loss: 2.2880\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 4s 40ms/sample - loss: 2.2768\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 4s 40ms/sample - loss: 2.2943\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 4s 43ms/sample - loss: 2.2839\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 4s 43ms/sample - loss: 2.2724\n",
      "20/20 [==============================] - 0s 20ms/sample - loss: 2.2683\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "X_train = np.random.random((100, 100, 100, 3))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "X_test = np.random.random((20, 100, 100, 3))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "def cnn_model() :   \n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation=tf.nn.relu, input_shape=(100, 100, 3)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation=tf.nn.relu))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (3, 3), activation=tf.nn.relu))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation=tf.nn.relu))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256, activation=tf.nn.relu))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = cnn_model()\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=10)\n",
    "score = model.evaluate(X_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (LSTM) for Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14480, 4)\n",
      "(14480,)\n",
      "(7133, 4)\n",
      "(7133,)\n",
      "(14480, 1, 4)\n",
      "(14480,)\n",
      "(7133, 1, 4)\n",
      "(7133,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print (X_test.shape)\n",
    "print (y_test.shape)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0],1,X_test.shape[1])\n",
    "\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print (X_test.shape)\n",
    "print (y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14480 samples, validate on 7133 samples\n",
      "Epoch 1/10\n",
      "14480/14480 [==============================] - 5s 372us/sample - loss: 416597886177.1669 - accuracy: 0.0000e+00 - val_loss: 446470601720.6785 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "14480/14480 [==============================] - 3s 232us/sample - loss: 416585115093.5690 - accuracy: 0.0000e+00 - val_loss: 446458183751.2048 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "14480/14480 [==============================] - 4s 253us/sample - loss: 416572870434.2277 - accuracy: 0.0000e+00 - val_loss: 446445782021.0245 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "14480/14480 [==============================] - 4s 247us/sample - loss: 416560566032.1238 - accuracy: 0.0000e+00 - val_loss: 446433338799.1050 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "14480/14480 [==============================] - 4s 244us/sample - loss: 416548345939.7304 - accuracy: 0.0000e+00 - val_loss: 446420902642.5414 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "14480/14480 [==============================] - 4s 244us/sample - loss: 416536053359.4519 - accuracy: 0.0000e+00 - val_loss: 446408501270.6822 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "14480/14480 [==============================] - 3s 231us/sample - loss: 416523762969.7415 - accuracy: 0.0000e+00 - val_loss: 446396042186.1657 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "14480/14480 [==============================] - 3s 239us/sample - loss: 416511456395.1735 - accuracy: 0.0000e+00 - val_loss: 446383617165.1176 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "14480/14480 [==============================] - 3s 238us/sample - loss: 416499227341.3658 - accuracy: 0.0000e+00 - val_loss: 446371173855.9147 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "14480/14480 [==============================] - 3s 229us/sample - loss: 416486942636.2696 - accuracy: 0.0000e+00 - val_loss: 446358766806.0452 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[120.20932],\n",
       "       [120.20932],\n",
       "       [120.20932],\n",
       "       ...,\n",
       "       [120.20932],\n",
       "       [120.20932],\n",
       "       [120.20932]], dtype=float32)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=1)\n",
    "\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./Graph',\n",
    "                                 write_graph=True,\n",
    "                                 histogram_freq=1)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "def lstm_model() :   \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.LSTM(128,input_shape=(X_train.shape[1], X_train.shape[2]),return_sequences=True))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.LSTM(32))\n",
    "    model.add(layers.Dense(1))  \n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),loss = \"mse\",metrics =[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = lstm_model()\n",
    "model.fit(X_train,y_train,epochs=10,validation_data=(X_test,y_test),callbacks=[tensorboard_callback])\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microsofts Model Interpretation (Detecting Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<interpret.glassbox.linear.LinearRegression at 0x16041add8>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from interpret.glassbox import LinearRegression\n",
    "\n",
    "ebm = LinearRegression()\n",
    "ebm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <iframe src=\"http://127.0.0.1:7498/5622542856/\" width=100% height=800 frameBorder=\"0\"></iframe>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from interpret import show\n",
    "\n",
    "ebm_global = ebm.explain_global()\n",
    "show(ebm_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <iframe src=\"http://127.0.0.1:7498/5918273944/\" width=100% height=800 frameBorder=\"0\"></iframe>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ebm_local = ebm.explain_local(X_test, y_test)\n",
    "show(ebm_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
       "0      5650     1.0           0     0  ...      7        1180              0   \n",
       "1      7242     2.0           0     0  ...      7        2170            400   \n",
       "2     10000     1.0           0     0  ...      6         770              0   \n",
       "3      5000     1.0           0     0  ...      7        1050            910   \n",
       "4      8080     1.0           0     0  ...      8        1680              0   \n",
       "\n",
       "   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "0      1955             0    98178  47.5112 -122.257           1340   \n",
       "1      1951          1991    98125  47.7210 -122.319           1690   \n",
       "2      1933             0    98028  47.7379 -122.233           2720   \n",
       "3      1965             0    98136  47.5208 -122.393           1360   \n",
       "4      1987             0    98074  47.6168 -122.045           1800   \n",
       "\n",
       "   sqft_lot15  \n",
       "0        5650  \n",
       "1        7639  \n",
       "2        8062  \n",
       "3        5000  \n",
       "4        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xai.data\n",
    "df = xai.data.pd.read_csv(\"kc_house_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0616 02:36:43.755062 4540274112 __init__.py:991] No categorical_cols passed so inferred using np.object, np.int8 and np.bool: Index(['date'], dtype='object'). If you see an error these are not correct, please provide them as a string array as: categorical_cols=['col1', 'col2', ...]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ims = xai.imbalance_plot(df, \"grade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bal_df = xai.balance(df, \"grade\", upsample=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
